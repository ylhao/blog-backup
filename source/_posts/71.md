---
title: 深度残差网络（ResNet）
categories: 机器学习
tags: 机器学习
mathjax: true
date: 2018-05-25 13:40:42
---

先上一张图~
![](/img/resnet01.png)

## 深度神经网络的退化问题

深度残差网络是2015年提出的深度卷积网络，一经出世，便在ImageNet中斩获图像分类、检测、定位三项的冠军。 

从经验来看，网络的深度对模型的性能至关重要，当增加网络层数后，网络可以进行更加复杂的特征模式的提取，所以当模型更深时理论上可以取得更好的结果，但是实验发现深度网络出现了退化问题（Degradation problem）。网络深度增加时，网络准确度出现饱和，甚至出现下降。如下图所示：

![](/img/resnet1.png)

可以看出56层的网络比20层网络效果还要差。但是这不是过拟合的问题，因为56层网络的训练误差同样高。

## 只做恒等映射也不该出现退化问题

深度网络的退化问题至少说明深度网络不容易训练。但是考虑以下事实：现在已经有了一个浅层神经网络，通过向上堆积新层来建立深层网络。一个极端情况是这些增加的层什么也不学习，仅仅复制浅层网络的特征，即向上堆积的层仅仅是在做恒等映射（Identity mapping）。在这种情况下，深层网络应该至少和浅层网络性能一样，也不应该出现退化现象。

## 恒等映射不是想学就能学

但是问题可能是，网络并不是那么容易的就能学到恒等映射。随着网络层数不断加深，求解器不能找到解决途径。

## ResNet
ResNet 就是通过显式的修改网络结构，加入残差通路，让网络更容易的学习到恒等映射。通过改进，我们发现深层神经网络的性能不仅不比浅层神经网络差，还要高出不少。

### Plaint net

![](/img/resnet2.png)

图中的 $H(x)$ 代表的是我们最终想要得到的一个映射。在 Plaint net 中，我们就是希望这两层网络能够直接拟合出 $H(x)$。

### Residual net

![](/img/resnet3.png)

图中的 $H(x)$ 代表的仍然是我们最终想要得到的一个映射。与 Plain net 不同的是，这里通过一个捷径连接（shortcut connections）直接将 $x$ 传到了后面与这两层网络拟合出的结果相加，$H(x)$ 是我们最终想要得到的一个映射，假设这两层网络拟合出来的映射为 $F(x)$，那么 $F(x)$ 应该等于 $H(x) - x$。

### 做这种变换的作用

如果 x 已经是最优的了，也就是说我们希望得到的映射 $H(x)$ 恰好就是此时的输入 $x$，也就是说要做恒等映射，这个时候只需要将权重值设定为 0。也就是让 $F(x) = 0$ 就好了。我们发现这比直接学习 $H(x) = x$ 要容易的多。

实际上残差网络相当于将学习目标改变了，学习的不再是一个完整的输出，而是目标值 $H(x)$ 和 x 的差值，也就是这篇文章一直在讨论的残差 $F(x)$。并且有 $F(x) = H(x) - x$。


### Residual Block

残差网络（Residual Networks）由许多隔层相连的神经元子模块组成，我们称之为 Residual Block。

### 34层的ResNet结构图

![](/img/resnet4.png)

### 图中的虚线

![](/img/resnet5.png)

经过捷径连接（shortcut connections）后，$H(x) = F(x) + x$，如果 $F(x)$ 和 $x$ 的通道数相同，则可直接相加。但是如果二者通道数不同，那么就不可以直接相加了。上图中的实线和虚线就是为了区分这两种情况：
1. 实线：表示二者通道数相同，二者可以直接相加。
2. 虚线：表示二者通道数不同，此时需要采用的计算方式为 $H(x)=F(x) + Wx$，其中 $W$ 代表卷积操作，用来调整 $x$ 的通道数。

### 两种残差学习单元

![](/img/resnet6.png)

两种结构分别针对ResNet34（左图）和 ResNet50/101/152（右图）。右图的主要目的是减少参数数量。

为了做个详细的对比，我们这里假设左图的残差单元的输入不是 64-d 的，而是 256-d 的，那么左图应该为两个 $3 \times 3, 256$ 的卷积。参数总数为：

$$
3 \times 3 \times 256 \times 256 \times 2 = 1179648
$$

对上式做个说明 $3 \times 3 \times 256$ 计算的是每个 filter 的参数数目，第 2 个 256 是说每层有 256 个filter，最后一个 2 是说一共有两层。

右图的输入同样为 256-d 的，首先通过一个 $1 \times 1, 64$ 的卷积层将通道数降为 64。然后是一个 $3 \times 3, 64$ 的卷积层。最后再通过一个 $1 \times 1, 256$ 的卷积层通道数恢复为 256。参数总数为：

$$
1 \times 1 \times 256 \times 64 + 3\times 3 \times 64 \times 64 + 1 \times 1 \times 64 \times 256 = 69632
$$

可见参数数量明显变少了。

通常来说对于常规的ResNet，可以用于34层或者更少的网络中（左图）；对于更深的网络（如101层），则使用右图，其目的是减少计算和参数量。

### 不同深度的ResNet

![](/img/resnet7.png)

### 参考
1. [大话深度残差网络（DRN）ResNet网络原理](https://my.oschina.net/u/876354/blog/1622896)
2. [秒懂！何凯明的深度残差网络PPT是这样的|ICML2016 tutorial](https://www.leiphone.com/news/201608/vhqwt5eWmUsLBcnv.html)
3. [深度残差网络（ResNet）浅析](https://blog.csdn.net/scety/article/details/52957991)

