---
title: 局部响应归一化（Local Response Normalization）
categories: 机器学习
tags: 机器学习
mathjax: true
date: 2019-05-13 13:51:30
---

LRN全称为Local Response Normalization，即局部响应归一化层，LRN函数类似Dropout，是一种防止过拟合的方法。这个函数很少使用，基本上被类似Dropout这样的方法取代，见最早的出处AlexNet论文对它的定义, [《ImageNet Classification with Deep ConvolutionalNeural Networks》](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)。

$b_{x,y}^{i} = \frac{a_{x,y}^{i}}{(k+\alpha\sum_{j=max(0, i-n/2)}^{min(N-1, i+n/2)}(a_{x,y}^{j})^2)^{\beta}} $

$b_{x,y}^{i}$是归一化后的值，i是通道的位置，代表更新第几个通道的值，x与y代表待更新像素的位置。

$a_{x,y}^{i}$是输入值，是激活函数Relu的输出值。

$k, \alpha, \beta, n/2$都是自定义系数。

N是总的通道数。

累加多少个通道的像素值取决于自定义系数 n/2 。

比如要计算红框位置的$b_{x,y}^{i}$，累加效果如图所示：

![](https://upload-images.jianshu.io/upload_images/9966001-fb0b9f3c1a21591b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## 参考

[LRN ( Local Response Normalization) 局部响应归一化层](https://blog.csdn.net/u014296502/article/details/78839881)



