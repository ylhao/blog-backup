## FM(Factorization Machines)

FM主要目标是：解决数据稀疏的情况下，特征怎样组合的问题。

FM一共有3个优点：
- 可以在非常稀疏的数据中进行合理的参数估计
- FM模型的时间复杂度是线性的
- FM是一个通用模型，它可以用于任何特征为实值的情况

在一般的线性模型中，各个特征相互独立，不考虑特征与特征之间的相互关系。但实际上，大量的特征之间是有关联的。 一般的线性模型为：

$y = w_0 + \Sigma_{i=1}^{n}w_{i} x_{i}$

为了表述特征间的相关性，可以采用多项式模型。在多项式模型中，特征$x_i$与$x_j$的组合用$x_{i} x_{j}$表示。为了简单起见，我们讨论二阶多项式模型。

$y = w_0 + \Sigma_{i=1}^{n}w_{i} x_{i} + \Sigma_{i=1}^{n}\Sigma_{j=i+1}^{n}w_{ij}x_{i}x_{j}$

该多项是模型与线性模型相比，多了特征组合的部分，特征组合部分的参数有$\frac{n(n-1)}{2}$个。如果特征非常稀疏且维度很高的话，时间复杂度将大大增加。 

对每一个特征，引入辅助向量lantent vector $V_{i}=[v_{i1},v_{i2},...,v_{ik}]^T$, 模型修改如下：

$y = w_0 + \Sigma_{i=1}^{n}w_{i} x_{i} + \Sigma_{i=1}^{n}\Sigma_{j=i+1}^{n}<V_i, V_j>x_{i}x_{j}$

其中k是超参数，即lantent vector的维度，一般取30或40，也可以取其他数，具体情况具体分析。使用$<V_i, V_j>$取代$w_{ij}$能够更好的学习到特征之间的相关性，并且能够减少模型中的参数数量，FM的二次项参数数量为nk。计算上式的时间复杂度为$O(kn^2)$，但是可以通过化简将时间复杂度降为$O(kn)$。

## FFM（Field-aware Factorization Machines）

FFM是FM的升级版模型，引入了field的概念。FFM把相同性质的特征归于同一个field。在FFM中，每一维特征$x_i$，针对每一种field $f_j$，都会学习到一个隐向量$V_{i,f_j}$，因此，隐向量不仅与特征相关，也与field相关。

设样本一共=有n个特征, f 个field，那么FFM的二次项有nf个隐向量。而在FM模型中，每一维特征的隐向量只有一个。FM可以看做FFM的特例，即把所有特征都归属到同一个field中。

$y = w_0 + \Sigma_{i=1}^{n}w_{i} x_{i} + \Sigma_{i=1}^{n}\Sigma_{j=i+1}^{n}<V_{i,f_{j}}, V_{j,f_{i}}>x_{i}x_{j}$

如果隐向量的长度为k，那么FFM的二次项参数数量为nfk，远多于FM模型。此外由于隐向量与field相关，FFM二次项并不能够化简，时间复杂度为$O(kn^2)$。需要注意的是由于FFM中的latent vector只需要学习特定的field，所以通常$K_{FFM} << K_{FM}$。

## 参考
- [CTR预估算法之FM, FFM, DeepFM及实践](https://blog.csdn.net/john_xyz/article/details/78933253)
- [FM系列算法解读（FM+FFM+DeepFM）](https://blog.csdn.net/hiwallace/article/details/81333604)