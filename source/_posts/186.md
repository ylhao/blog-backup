---
title: 线性回归（一）
categories: 机器学习
tags: 机器学习
mathjax: true
date: 2018-05-13 08:55:28
---

## 符号定义
1. $m$：训练样本数目
2. $(x^{(i)}, y^{(i)})$：第 i 个样本

## 假设函数
$$
h(x) = \sum_{i=1}^n \theta _i x_i = \theta^T x
$$

## 目标函数
$$
J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)})-y^{(i)}) ^ 2
$$

## 对目标函数的解释
“最小二乘法”的核心就是保证所有数据偏差的平方和最小。为什么要让模型的预测数据与实际数据之差的平方而不是绝对值和最小来优化模型参数？这里做以下推导说明。

对于每一个样例$(x^{(i)}, y^{(i)})$，假设预测值和真实值存在以下关系：
$$
y_{(i)} = \theta^T x^{(i)} + \epsilon^{(i)}
$$
其中$\epsilon^{(i)}$ 表示预测值和真实值之间的差值。基于这个假设进一步假设:
$$
\epsilon \sim N(0, \sigma^2)
$$
因为这个误差受很多因素的影响，这些因素都是随机分布的，根据中心极限定理，许多随机变量的和趋于正态分布。可以知道假设二是合理的。
$$
P(\epsilon^{(i)}) = \frac{1}{\sqrt{2 \pi}} \sigma exp \Big(- \frac {(\epsilon^{(i)})^2}{2 \sigma ^ 2} \Big)
$$
当给定参数 $\theta$ 和变量 $x$ 时，有以下式成立：
$$
P(y^{(i)} | x^{(i)};\theta) = \frac{1}{\sqrt{2\pi} \sigma} exp \Big(- \frac {(\theta^T x^{(i)}-y{(i)})^2}{2 \sigma ^ 2} \Big)
$$
进一步假设各个样例的误差独立同分布，可以得到似然函数：
$$
\begin{aligned}
l(\theta) =& P(Y|X; \theta) \\
=& \prod_{i=1}^m exp \Big(- \frac {(\theta^T x^{(i)}-y{(i)})^2}{2 \sigma ^ 2} \Big)
\end{aligned}
$$
似然函数表示的是在参数 $\theta$ 下，数据集出现的概率。我们这里要做的工作就是找到让数据集出现概率最大的参数 $\theta$，也就是极大似然估计。将似然函数转化为对数似然：
$$
L(\theta) = log l(\theta) = -m log \sqrt{2 \pi} \sigma - \frac{1}{2\sigma^2} \sum_{i=1}^m (\theta^T x^{(i)}-y{(i)})^2
$$
到这里我们已经推导出了目标函数的形式。


## 批梯度下降
$$
\theta_j := \theta_j \alpha \frac{\partial}{\partial \theta_j} J(\theta) \\
\theta_j := \theta_j \alpha \frac{1}{2m} \sum_{i=1}^m (h_{\theta}(x^{(i)})- y^{(i)})x_j^{(i)}
$$


## 随机梯度下降
当每次只用一个样本进行训练时，$J(\theta)$ 退化成以下形式：
$$
J(\theta) = (h_{\theta}(x^{(i)})-y^{(i)}) ^ 2
$$
此时参数更新公式变为以下形式：
$$
\theta_j := \theta_j \alpha \frac{\partial}{\partial \theta_j} J(\theta) \\
\theta_j := \theta_j \alpha (h_{\theta}(x^{(i)})- y^{(i)})x_j^{(i)}
$$
随机梯度下降（stochastic gradient descent）可能永远不能收敛到最小值，参数 $\theta$ 将会一直在使 $J(\theta)$取最小值的附近振荡，但实际情况时，当数据量足够大的时候，这已经足够了。

## 参考
1. 斯坦福机器学习公开课 —— 吴恩达
2. 斯坦福ML公开课笔记 —— 张雨石

