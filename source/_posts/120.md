---
title: 循环神经网络：LSTM
categories: 机器学习
tags: 机器学习
mathjax: true
date: 2018-05-24 13:21:40
---

LSTM(Long Short Term Memories) 是一类特殊的循环神经网络结构，其隐藏层有着特殊的结构，给出了一种计算隐藏层状态的新方法。通过引入门控机制来解决 RNN 的梯度消失问题，从而能够学习到长距离依赖。

我们之前分析了 RNN 的简单结构，其结构大致如下图所示：

![](/img/lstm1.png)

下面给出 LSTM 的基本结构：

![](/img/lstm2.png)

下面介绍一下图中的各个符号的语义：

![](/img/lstm3.png)

各个符号的语义如下：
- Neural NetWork Layer：该图表示一个神经网络层
- Pointwise Operation：该图表示一种操作，如加号表示矩阵或向量的求和、乘号表示向量的乘法操作
- Vector Tansfer：每一条线表示一个向量，从一个节点输出到另一个节点
- Concatenate：该图表示两个向量的合并，即由两个向量合并为一个向量，如有 $x_1$ 和 $x_2$ 两向量合并后为 $[x_1,x_2]$ 向量
- Copy：该图表示一个向量复制了两个向量，其中两个向量值相同

## 对 RNN 简单结构的解释

为了接下来叙述的更清楚，先描述一下 RNN 的简单结构。
1. $x_t$ 为 t 时刻的输入
2. $h_t$ 首先为 t 时刻的隐藏层状态，同时也为 t + 1 时刻的输入，这里还直接把 $h_t$ 作为输出而不做进一步处理

那么我们可以理解成在时刻 t， $h_{t-1}$ 和 $x_t$ 作为一个激活函数为 tanh 的神经网络层的输入，通过计算得到了当前时刻的隐藏层状态 $h_t$，这个 $h_t$ 既是下一时刻的输入，也是当前时刻的输出。

## 分析 LSTM

LSTM最核心的部分是**cell state**。时刻 t 对应的 cell state 是 $C_t$。如下图中的直线所示 cell state 贯穿所有时刻。

![](/img/lstm4.png)

### LSTM 中的门

在前向传播的过程中，通过**门**来控制 $c_t$ 中信息的增减。LSTM中的门是通过一个激活函数为sigmoid的神经网络层来实现的，门的输出值在 $0 \sim 1$ 之间。然后把门的取值向量和目标数据按位相乘就可以达到控制数据流通的效果。LSTM中共有三个门，分别为**forget gate**，**input gate**，**output gate**。这三个门的计算方法公式一样，都是根据 $x_t$ 和 $h_t−1$ 来计算， 区别在于这三个门对应的神经网络层的权重矩阵和偏置不同。

### forget gate $f_t$

首先考虑从上一时刻的 cell state 中丢弃什么信息，这由 forget gate 来控制。

![focus forget gate](/img/lstm5.png)

### input gate $i_t$

接着考虑当前时刻的新信息（candidate values, $\widetilde{C}_t$）有哪些需要添加到 cell state。

![focus input gate](/img/lstm6.png)

从公式中可以看出 candidates values($\widetilde{C}_t$) 的计算方式就是简单的 RNN 结构中的隐藏层状态的计算方式。

计算出 $i_t$ 和 $\widetilde{C}_t$ 后，$i_t * \widetilde{C}$，就是要添加到 cell state 中的新信息，其中 \* 代表两个向量按位乘。

### 对以上两个门的作用的总结

通过 forget gate 和 input gate 这两个门的控制作用，我们已经丢弃了 cell state 中该丢弃的那部分信息，并且向 cell state 中添加了该添加的新信息。图中的 $*$ 表示按位乘。

![](/img/lstm7.png)

### output gate $o_t$

通过之前的计算，我们已经得到了当前时刻的 cell state（$C_t$），接下来我们考虑通过当前的 cell state 产生输出。通过 output gate 来控制 当前时刻的 cell state 有哪些信息应该被输出。图中的 $*$ 表示按位乘。

![](/img/lstm8.png)


至此我们就介绍完了LSTM。在 LSTM 循环神经网络结构中，在每个时刻存在着一些相互交互的神经网络层，其中的一些网络层起到了门控的作用，控制了信息的流通。

## 参考
1. [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
2. [CS224d笔记4续——RNN隐藏层计算之GRU和LSTM](https://wugh.github.io/posts/2016/03/cs224d-notes4-recurrent-neural-networks-continue/)
3. 深度学习工程师微专业 —— 吴恩达

