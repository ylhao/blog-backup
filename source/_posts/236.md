---
title: 超参调节方法
categories: 机器学习
tags: 机器学习
mathjax: true
date: 2018-05-21 19:31:51
---

## 神经网络的常见超参数
神经网络需要调节的超参数大致有以下几个：
- $\alpha$：学习率
- $\beta$：动量梯度下降因子
- $\beta_1, \beta_2, \varepsilon$：Adam 优化算法参数
- #layer：神经网络层数
- #hidden units：各隐藏层的神经元数目
- learning rate decay：学习率衰减参数
- mini-batch size：批梯度下降法，每个 batch 的大小

## 基本的调参原则
1. 通常来说，学习率 $\alpha$ 是最重要的超参数，也是需要重点调试的超参数。
2. 动量梯度下降因子 $\beta$、各隐藏层神经元个数 #hidden units 和 mini-batch size 的重要性仅次于 $\alpha$。
3. 接下来比较重要的就是神经网络的层数 #layer 和学习率衰减参数 learning rate decay。
4. 最后 Adam 优化算法的三个参数 $\beta_1, \beta_2$ 和 $\varepsilon$ 一般设置为 0.9，0.999 和 $10^{-8}$，并不需要反复调试。
5. 当然，这里超参数重要性的排名并不是绝对的，具体情况，具体分析。

## 参考
1. 深度学习工程师微专业 —— 吴恩达
2. [吴恩达《优化深度神经网络》精炼笔记（3）-- 超参数调试、Batch正则化和编程框架](https://mp.weixin.qq.com/s?__biz=MzIwOTc2MTUyMg==&mid=2247483961&idx=1&sn=6e369887d6c76b84a5eb48e8a7cd512f&chksm=976fa7a4a0182eb2d37c178ace5b961eba574f1006c74db11516691a52abbcbbd97862a932ea&scene=21#wechat_redirect)

