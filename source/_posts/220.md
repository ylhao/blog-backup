---
title: 主成分分析（PCA）
categories: 机器学习
tags: 机器学习
mathjax: true
date: 2018-05-08 00:14:41
---

主成分分析（Principal Components Analysis, PCA）是一种降维方法。这种方法比较简单直接，主需要计算特征向量的特征值就可以进行降维。

## 准备
1. 数据集 $\{x^{(i)}; i=1,2, \dots, m\}$，其中 $x^{(i)} \in \mathbb {R}^n $

## 预处理
运行 PCA 算法之前一般都要先进行预处理：
1. $ \mu = \frac{1}{m} \sum_{i=1}^{m} x^{(i)}  $
2. $ x^{(i)} = x^{(i)} - \mu $
3. $ \sigma_j^2 = \frac{1}{m} \sum_i(x_j^{(i)})^2 $
4. $ x_j^{(i)} = \frac{x_j^{(i)}}{\sigma_j}$

第 1、2 两步将数据的均值变成 0，当数据的均值本来就是 0 的时候可以省略。第 3 步是求数据在各个维度上的方差，第 4 步是将数据的各个维度的方差变为 1，这样就使得数据在各个维度都在同一尺度下被衡量。


## 如何找主轴方向？
假设找到了主轴方向，我们希望数据在主轴方向上能够被更好的区分开，直观的说就是我们希望数据在主轴上尽量分散。更具体的就是指所有的点在主轴方向的投影点的方差最大。比如在以下两个图中，在方向一上，数据更分散，投影点的方差最大，所以如果从这两个方向上选一个主轴的话，应该选方向一。
![方向一](/img/pca_1.png)
![方向二](/img/pca_2.png)

注意我们已经将数据标准化了，数据的均值为0，其投影点的均值也为0，所以计算方差的时候，直接平方就好。
$$
max \frac{1}{m} \sum_{i=1}^{m}(x^{(i)^T}u)^2 \\
max \frac{1}{m} \sum_{i=1}^{m} \mu^T x^{(i)} x^{(i)^T} \mu \\
max \mu^T \Big( \frac{1}{m} \sum_{i=1}^m x^{(i)} x^{(i)^T} \Big) \mu \\
$$
上面的第一个式子里的 $x^{(i)^T} \mu$ 就是 $x^{(i)}$ 这个向量再投影方向 $\mu$ 上的长度。$\mu$ 是单位向量，$\lVert \mu \rVert = 1$，这也是最大化这个表达式的约束条件。
上面的第二个式子是把向量内积的平方换了一个写法。
上面的第三个式子又对式子做了一个变形，不难看出 $ \frac{1}{m} \sum_{i=1}^m x^{(i)} x^{(i)^T} $ 是一个矩阵，并且这个矩阵是对称矩阵（实际上是一个协方差矩阵），最后再加上我们要求解的是 $\mu$，所以最终问题变成了一个求特征值和特征向量的问题。$\mu$ 就是要求解的特征向量。

## 证明 $\mu$ 就是上述所说矩阵的特征向量
可以使用拉格朗日方程来求解该最大化问题：
$$
\begin{aligned}
l =& \mu^T \Big( \frac{1}{m} \sum_{i=1}^m x^{(i)} x^{(i)^T} \Big) \mu - \lambda (\lVert \mu \rVert - 1)\\
=& \mu^T \Sigma \mu - \lambda(\mu^T \mu - 1)
\end{aligned}
$$
对 $\mu$ 求导：
$$
\begin{aligned}
\triangledown_{\mu} l =& \triangledown_{\mu}  \mu^T \Sigma \mu - \lambda \triangledown_{\mu} \mu^T \mu \\
=& \Sigma \mu - \lambda \mu
\end{aligned}
$$
令导数为 0，可以发现 $\mu$ 是 $\Sigma$ 的特征向量。

## 一定有特征向量？
再因为上述矩阵是对称矩阵，所以一定能找到 n 个相互正交的特征向量，我们从 n 个里面挑选 k 个最大的特征值对应的特征向量即可。
$$
\{\mu^1, \mu^2, \dots, \mu^n\}
$$

## $\Sigma$ 是什么？
其实 $\Sigma$ 就是要给协方差矩阵。

## 降维公式
通过以下公式将原来是 n 维的 x^{(i)} 降到了 k 维：
$$
[y_1^{(i)}, y_2^{(i)}, \dots, y_k^{(i)}] = [u_1^T x^{(i)}, u_2^T x^{(i)}, \dots, u_k^T x^{(i)}]
$$

## 参考
1. 斯坦福机器学习公开课 —— 吴恩达
2. 斯坦福ML机器学习公开课笔记 —— 张雨石

