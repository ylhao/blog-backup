---
title: Python：超简BP算法实现（原理再现）
date: 2018-08-31 09:26:04
categories: 编程
tags:
- Python
- 机器学习
---

![网络结构](/img/simple_bp.png)

```
# encoding: utf-8

import numpy as np

def sigmod(x, deriv=False):
    if deriv:
        return x * (1 - x)
    return 1 / (1 + np.exp(-x))


X = np.array([[0.35],[0.9]])  # 输入
y = np.array([[0.5]])  # 真实值

print("X={}".format(X))
print("y={}".format(y))
print("X.shape={}".format(X.shape))
print("y.shape={}".format(y.shape))

W0 = np.array([[0.1, 0.8], [0.4, 0.6]])
W1 = np.array([[0.3, 0.9]])  # W1 = np.array([0.3, 0.9])  # 如果这样定义，定义的是一个列向量，shape 为 (2,)

print("W0.shape={}".format(W0.shape))
print("W1.shape={}".format(W1.shape))

print("original W0={}".format(W0))
print("original W1={}".format(W1))

for j in range(100):
    l0 = X
    # 隐藏层的输出
    l1 = sigmod(np.dot(W0, l0))
    # 输出层的输出（预测值）
    l2 = sigmod(np.dot(W1, l1))
    # 反向传播
    l2_error = y - l2
    error = (y - l2)**2 / 2.0
    print("Loss={}".format(error))
    l2_delta = l2_error * sigmod(l2, deriv=True)
    l1_error = l2_delta * W1
    l1_delta = l1_error * sigmod(l1, deriv=True)
    #修改权值
    W1 += l2_delta * l1.T
    W0 += l0.T.dot(l1_delta)

print("W0={}".format(W0))
print("W1={}".format(W1))
```

```
X=[[0.35]
 [0.9 ]]
y=[[0.5]]
X.shape=(2, 1)
y.shape=(1, 1)
W0.shape=(2, 2)
W1.shape=(1, 2)
original W0=[[0.1 0.8]
 [0.4 0.6]]
original W1=[[0.3 0.9]]
Loss=[[0.0181039]]
Loss=[[0.01652628]]
Loss=[[0.01506159]]
Loss=[[0.0137064]]
Loss=[[0.01245646]]
Loss=[[0.01130686]]
Loss=[[0.01025225]]
Loss=[[0.00928702]]
Loss=[[0.00840541]]
Loss=[[0.00760166]]
Loss=[[0.0068701]]
Loss=[[0.00620521]]
Loss=[[0.00560171]]
Loss=[[0.00505455]]
Loss=[[0.00455898]]
Loss=[[0.00411052]]
Loss=[[0.00370503]]
Loss=[[0.00333862]]
Loss=[[0.00300775]]
Loss=[[0.00270911]]
Loss=[[0.00243969]]
Loss=[[0.00219674]]
Loss=[[0.00197772]]
Loss=[[0.00178034]]
Loss=[[0.00160251]]
Loss=[[0.00144233]]
Loss=[[0.00129808]]
Loss=[[0.00116819]]
Loss=[[0.00105124]]
Loss=[[0.00094598]]
Loss=[[0.00085122]]
Loss=[[0.00076595]]
Loss=[[0.0006892]]
Loss=[[0.00062014]]
Loss=[[0.00055799]]
Loss=[[0.00050207]]
Loss=[[0.00045175]]
Loss=[[0.00040648]]
Loss=[[0.00036574]]
Loss=[[0.00032909]]
Loss=[[0.00029612]]
Loss=[[0.00026645]]
Loss=[[0.00023976]]
Loss=[[0.00021574]]
Loss=[[0.00019413]]
Loss=[[0.00017469]]
Loss=[[0.00015719]]
Loss=[[0.00014145]]
Loss=[[0.00012729]]
Loss=[[0.00011455]]
Loss=[[0.00010308]]
Loss=[[9.27632745e-05]]
Loss=[[8.34798028e-05]]
Loss=[[7.51262967e-05]]
Loss=[[6.76094923e-05]]
Loss=[[6.08454893e-05]]
Loss=[[5.47588101e-05]]
Loss=[[4.92815538e-05]]
Loss=[[4.43526347e-05]]
Loss=[[3.9917098e-05]]
Loss=[[3.59255034e-05]]
Loss=[[3.23333711e-05]]
Loss=[[2.91006835e-05]]
Loss=[[2.6191437e-05]]
Loss=[[2.35732389e-05]]
Loss=[[2.12169445e-05]]
Loss=[[1.9096331e-05]]
Loss=[[1.71878044e-05]]
Loss=[[1.54701353e-05]]
Loss=[[1.39242217e-05]]
Loss=[[1.25328756e-05]]
Loss=[[1.12806307e-05]]
Loss=[[1.01535701e-05]]
Loss=[[9.1391704e-06]]
Loss=[[8.22616248e-06]]
Loss=[[7.40440534e-06]]
Loss=[[6.66477319e-06]]
Loss=[[5.99905368e-06]]
Loss=[[5.39985637e-06]]
Loss=[[4.86053034e-06]]
Loss=[[4.37509018e-06]]
Loss=[[3.93814926e-06]]
Loss=[[3.5448598e-06]]
Loss=[[3.19085892e-06]]
Loss=[[2.87222006e-06]]
Loss=[[2.58540935e-06]]
Loss=[[2.32724628e-06]]
Loss=[[2.09486834e-06]]
Loss=[[1.88569918e-06]]
Loss=[[1.69742002e-06]]
Loss=[[1.52794387e-06]]
Loss=[[1.37539232e-06]]
Loss=[[1.23807471e-06]]
Loss=[[1.11446931e-06]]
Loss=[[1.0032065e-06]]
Loss=[[9.03053486e-07]]
Loss=[[8.12900662e-07]]
Loss=[[7.31749283e-07]]
Loss=[[6.58700387e-07]]
Loss=[[5.92944818e-07]]
W0=[[0.0993309 0.6425254]
 [0.3993309 0.4425254]]
W1=[[-0.30032342  0.31508797]]
```

