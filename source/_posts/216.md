---
title: 循环神经网络：RNN简介、BPTT算法、梯度消失、梯度爆炸
categories: 机器学习
tags: 机器学习
mathjax: true
date: 2018-05-24 01:08:48
---

## 符号定义和解释

首先说明这里的推导采用的符号如下图所示：

![A recurrent neural network and the unfolding in time of the computation involved in its forward computation. Source: Nature](/img/bptt_1.png)

其中：
1. $x_t$ 是第 t 个时刻的输入
2. $s_t$ 是第 t 个时刻隐藏层的状态
3. $o_t$ 是第 t 个时刻的输出，比如如果我们想要预测下一个词是什么，那么可以认为 $o_t = softmax(Vs_t)$
4. $s_t$ 计算方式为 $s_t = f(Ux_t + Ws_{t-1})$，其中的函数 f 代表一个非线性函数，比如 tanh 或者 ReLU
5. 第 1 个时刻对应的输入 $s_0$ 通常初始化为零向量
6. $U,W,V$ 是循环神经网络的参数，所有时刻共享，这在很大程度上减少了参数数量

## 形象化理解 $s_t$
$s_t$ 是隐藏层的状态，可以把 $s_t$ 看成循环神经网络的记忆，通过 $s_t$ 可以知道在之前所有时刻都发生了什么，但是实际情况通常是通过 $s_t$ 并不能知道“很久以前”到底发生了什么。循环神经网络的最重要的一个特性就是隐藏层状态了，我们可以通过隐藏层状态捕获到一个序列的相关信息。

## 对 $o_t$ 的理解
$o_t$ 是神经网络的输出，如上图所示，每个时刻都有输出，但是实际上每个时刻是否需要有输出是视情况而定的。比如我们做文本情感分析，我们只关心整个句子最后一个时刻的输出，那之前所有的时刻都是不需要输出的。

## Backpropagation Through Time(BPTT)

首先我们有：
$$
s_t = tanh(Ux_t + W_s{t-1}) \\
\widehat{y}_t = softmax(Vs_t)
$$

接着我们可以用交叉熵来计算每个时刻的 loss：
$$
\begin{aligned}
E_t(y_t, \widehat{y}_t) = -y_t log \widehat{y}_t \\
\end{aligned}
$$

通常我们把每次输入的一个序列看成一个训练样本，所以就把每个时刻的交叉熵的和看成这个样本的总误差：
$$
\begin{aligned}
E(y, \widehat{y})
=& \sum_t E(y_t, \widehat{y}_t) \\
=& -\sum_t y_t log \widehat{y}_t
\end{aligned}
$$

首先我们根据以上的定义，给出以下简化了的图片：
![](/img/bptt_2.png)

接下来我们通过总误差计算 $U,V,W$ 这些参数的梯度，然后用随机梯度下降法更新参数。接下来以 $E_3$ 为例详细分析。

首先计算参数 $V$ 的梯度：

![](/img/bptt_3.png)

在以上推导过程中 $z_3 = Vs_3$，$\otimes$ 代表的是计算两个向量的外积（叉乘）。我们发现参数 $V$ 的梯度仅仅依赖当前时刻的隐层状态和当前时刻的输出。

接下来计算参数 $W$ 和参数 $U$ 的梯度，首先我们给出一张计算图，图中的矩形框代表的输入，图中的椭圆形框代表的计算函数。通过计算图我们可以得到参数 $W$ 和参数 $U$ 的梯度：

![](/img/bptt_4.png)

$$
\frac{\partial E_3}{\partial W} = \frac{\partial E_3}{\partial \widehat{y}_3} \frac{\partial \widehat{y}_3} {\partial s_3}\frac{\partial s_3}{\partial W} + 
\frac{\partial E_3}{\partial \widehat{y}_3} \frac{\partial \widehat{y}_3} {\partial s_3} \frac{\partial s_3} {\partial s_2} \frac{\partial s_2}{\partial W} +
\frac{\partial E_3}{\partial \widehat{y}_3} \frac{\partial \widehat{y}_3} {\partial s_3} \frac{\partial s_3} {\partial s_2} \frac{\partial s_2} {\partial s_1} \frac{\partial s_1}{\partial W}
$$

$$
\frac{\partial E_3}{\partial U} = \frac{\partial E_3}{\partial \widehat{y}_3} \frac{\partial \widehat{y}_3} {\partial s_3}\frac{\partial s_3}{\partial U} + 
\frac{\partial E_3}{\partial \widehat{y}_3} \frac{\partial \widehat{y}_3} {\partial s_3} \frac{\partial s_3} {\partial s_2} \frac{\partial s_2}{\partial U} +
\frac{\partial E_3}{\partial \widehat{y}_3} \frac{\partial \widehat{y}_3} {\partial s_3} \frac{\partial s_3} {\partial s_2} \frac{\partial s_2} {\partial s_1} \frac{\partial s_1}{\partial U}
$$

形象化的图像如下图所示：
![](/img/bptt_5.png)

我们可以将参数 W 的表达式简化成以下形式：
![](/img/bptt_6.png)

## RNN 中的梯度消失和梯度爆炸
首先我们从上式中可以看出，参数 $W$ 和参数 $U$ 的导数存在连乘的情况。通过以下公式

$$
s_t = tanh(Ux_t + Ws_{t-1})
$$

我们可以发现每次向前传递误差（每次求导）都要经过（处理）一个 tanh 函数。tanh 函数的图像和 tanh 函数的导函数图像如下图所示：

![](/img/bptt_7.png)

然后我们知道 tanh 函数的基本形式和其导数形式为：

$$
tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} \\
{tanh}'(x) = (1-tanh^2(x))
$$

下面展开 $ \frac{\partial s_3} {\partial s_2}$ 来更加具体的说明梯度消失和梯度爆炸的问题：
$$
s_3 = tanh(Ux_t + Ws_2) \\
\frac{\partial s_3}{\partial s_2} = {tanh}'(Ux_t + W s_2) W
$$

### 梯度消失

通过上图我们知道 tanh 函数的导数小于等于 1，同时如果参数 W 初始化的很小的话，那么 $\frac{\partial s_3} {\partial s_2}$ 将会是一个小于 0 的数，我们可以类推出 $\frac{\partial s_{t}} {\partial s_{t-1}}$ 将会是一个小于 0 的数。假设有 n 项连乘，则可形式化表示为：
$$
(小于0的数) ^ n
$$

随着时刻向前推移（误差沿着时间向前传递），梯度是呈指数级下降的。这也就是梯度消失问题。

### 梯度爆炸

如果 W 初始的很大的话，那么我们可以类推出 $\frac{\partial s_{t}}{\partial s_{t-1}}$ 将会是一个大于 0 的数。如果有 n 项连乘，则可形式化表示为：
$$
(大于0的数) ^ n
$$

随着时刻向前推移（误差沿着时间向前传递），梯度是呈指数级上升的。这也就是梯度爆炸的问题。

## 解决办法
梯度爆炸的问题可以用梯度裁剪的方式来缓解。
梯度消失的问题则有以下缓解方式：
1. 更换激活函数，比如可以选择 ReLU 函数。
2. 更改 RNN 隐藏层的结构，比如采用 GRU 或者 LSTM 的隐藏层结构。

## 参考
1. [Recurrent Neural Networks Tutorial, Part 1 – Introduction to RNNs](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) 
2. [Recurrent Neural Networks Tutorial, Part 3 – Backpropagation Through Time and Vanishing Gradients](http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/)
3. [深度学习中的激活函数与梯度消失](http://www.cnblogs.com/willnote/p/6912798.html)

