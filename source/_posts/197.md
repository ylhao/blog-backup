---
title: KNN算法（二）：kd-tree
date: 2018-05-04 16:00:42
categories: 机器学习
tags: 机器学习
mathjax: true
---

这篇文章介绍 K 近邻算法的一种实现：kd树

## 为什么要构造 kd 树？
K 近邻算法的最简单实现就是线性扫描，即输入一个实例，则依次计算这个实例与训练集中的每个实例的距离，然后选取前 K 个。这种方法计算量太大，特别是特征空间维数很高或者数据量很大时，这种计算将变得非常耗时。
可以考虑用特殊的数据结构来存储训练数据，以减少计算距离的次数。kd 树就是这么一种特殊的数据结构，kd 树是一棵二叉树。

## 构造 kd 树
### (1) 构造过程
构造 kd 树的过程相当于不断用垂直于坐标轴的超平面将 k 维空间进行切分，最后的结果是每个点都在分割超平面上。这里直接给出书上的例子。
设有以下数据集：
$$
T = \{(2, 3)^T,(5, 4)^T,(9, 6)^T,(4, 7)^T,(8, 1)^T,(7, 2)^T\}
$$
首先选择 $ x^{(1)}$ 轴，这 6 个数据点在 $ x^{(1)}$ 轴上的值分别为 $2, 5, 9, 4, 8, 7$，中位数为 7，所以根节点选 $(7, 2)^T$，并且 $(2, 3)^T,(5, 4)^T,(4, 7)^T$ 划分到根节点的左子树，$(9, 6)^T, (8, 1)^T$ 划分到根节点的右子树。接下来就是分别构造左右子树，定义此时要构造的结点对应的深度为 $j = 1$。我们会根据这个深度计算接下来划分要选择的轴。计算公式如下：
$$
l = jmodk + 1
$$
其中 $l$ 为要选择的轴，$k$ 为特征数（特征空间的维度）。其实这个计算过程并不复杂，例如假设特征数为 5，那么在构建整棵树的根节点时选用的是第 1 个特征（轴$x^{(1)}$），按照这种计算方式，那么接下来构造左右子树，选择的特征就是第 2 个特征（轴$x^{(2)}$），递归下去依次为 $3, 4, 5, 1, 2, \cdots$。

### (2) 递归结束条件
整个递归过程什么时候停止呢？
数据集中的所有实例与树中的结点一一对应时递归也就结束了，递归结束的更一般的条件描述为：当一个结点两个子区域（子结点）不再有实例可以被划分时，递归结束。

### (3) 构造结果
kd 树结果如下：
![kd 树构造结果](/img/knn2_1.png)
在空间上的划分结果如下：
![特征空间划分结果](/img/knn2_2.png)

## 搜索 kd 树
以最邻近算法为例，kd 树的搜索过程在最优情况下复杂度为 Olog(N)，在最坏情况下会退化成 O(N)。假定目标实例为 $K = (8, 3)$，下面给出 kd 树的搜索过程（更详细的搜索过程可以直接参考列出的参考资料 2）。
（1）首先经过比较到达结点 E，首先会认为此叶子结点为“当前最近点”，并计算结点 K 和结点 E 之间的距离为 $2$，记作 KE，KE 也就是当前最短距离。
（2）回溯到了 C 结点，计算 K 结点和 C 结点之间的距离为 $\sqrt{10}$，记作 KC，KC > 当前最短距离，所以“当前最近点”不用更新。
（3）接下来我们要判断的是当前回溯到的结点的另一个子结点对应的区域是否有可能有更近的点。当前点为 C 结点，其实此时就是要判断一下以目标点 K 为圆心，以“当前最近距离”为半径画圆，看看这个圆是否与结点 C 所在的切割面相交，只要相交，就说明在 C 的另一个子节点对应的区域中，可能有结点到目标结点 K 的距离最小。这里判断没有相交，继续向前回溯。
（4）回溯到了结点 F，计算 F 结点和 目标节点 K 之间的距离为 $\sqrt{2}$，记作 KF，KF < 当前最短距离，所以更新当前最近点为 F，更新当前最短距离为 KF。
（5）以目标结点 K 为圆心，当前最短距离为半径画圆，发现与 F 所在的分割超平面有交点（$8 - \sqrt{2} < 7$）。所以需要搜索 F 结点的左子树。
（6）经过比较（这里比较的是 K 结点和 B 结点的第二个特征对应的值），到达 A 结点。然后计算 A 结点 和目标结点 K 的距离，大于当前最短距离，并且以目标点为圆心，最短距离为半径画圆也不和 A 结点所在的分割超平面有交点。
（7）回溯到 B 结点，计算 B 结点和目标结点 K 之间的距离，大于当前最短距离，并且以目标点为圆心，最短距离为半径画圆，和 B 结点所在的分割超平面没有交点。
（8）向前回溯到 F 结点，递归结束，最后得到最近邻为 F 结点。

## K 近邻算法的优点
- 易于实现，没有显示式的学习过程，易于理解。
- K 近邻算法是一种在线算法，新数据可加入数据集而不必重新训练。

## K 近邻算法的缺点
- 样本不均衡时，偏向包含样本数目多的类别（可以通过加权的方式来进行改进，距离近的权值大，距离小的权值小）。
- 计算量大（kd树是针对这个问题的一个改进方法）。

## 参考
1. 《统计学习方法》 —— 李航
2. [完结篇|一文搞定k近邻算法（k-NN）算法（二）](https://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&mid=2247483793&idx=1&sn=d42f4f06225cd1f792912dfecdc800ea&chksm=ebb43945dcc3b0538ba10187c999a050137b96a05f402b9fe99f0a972d7ba0bce5067bf7efb3&scene=21#wechat_redirect) —— 忆臻
