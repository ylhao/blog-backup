---
title: sklearn：SVC算法和随机森林算法实战
categories: 编程
tags:
  - 编程
  - scikit-learn
date: 2018-06-12 13:09:49
---

``` python
# encoding: utf-8


from sklearn.svm import SVC
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import roc_curve
from sklearn.metrics import auc
from sklearn.metrics import recall_score,precision_score,f1_score,classification_report
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import confusion_matrix
from sklearn.ensemble import RandomForestClassifier
import pickle


class RF:

    """随机森林"""

    def __init__(self):

        # 对参数 n 进行寻参，这里的参数范围是根据实际情况定义的
        self.n_estimators_options = [5 * i for i in range(1, 40)]
        self.best_n_estimators = 0
        self.best_acc = 0

    def train(self, X, y, TEST_x, Test_y):
        """
        X: 训练集
        y: 训练集标签
        Test_x: 测试集
        Test_y: 测试集标签
        """
        # 把训练集划分成训练集和验证集两部分，用来寻参
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)
        # 寻参
        for n_estimators_size in self.n_estimators_options:
            alg = RandomForestClassifier(n_jobs=-1, n_estimators=n_estimators_size)
            alg.fit(X_train, y_train)
            predict = alg.predict(X_test)
            acc = (y_test == predict).mean()
            # 更新最优参数和 acc
            if acc >= self.best_acc:
                self.best_acc = acc
                self.best_n_estimators = n_estimators_size
            print('[n_estimators, acc]:', n_estimators_size, acc)
        # 用最优参数进行训练
        rf = RandomForestClassifier(n_jobs=-1, n_estimators=self.best_n_estimators)
        rf.fit(X, y)
        # 预测标签
        predict = rf.predict(TEST_x)
        acc = (Test_y == predict).mean()
        print('acc:', acc)


#输入特征向量文件，输出一个feature_list[[],[]~]及一个labellist[,,~]
def loadDataSet(file_path):
    """这个函数不用太关心，作用是构造数据集"""
    dataMat = []
    labelMat = []
    fileHandle = open(file_path,'r')
    fileContent = fileHandle.readlines()
    for line in fileContent:
        lineArr = line.strip().split()
        a = []
        for i in range(len(lineArr)-1):
            a.append(float(lineArr[i]))
        dataMat.append(a)
        labelMat.append(int(lineArr[len(lineArr)-1]))
    return dataMat,labelMat


def ml_svc(X, y, Test_x, Test_y):
    """
    svc 算法
    X: 训练集
    y: 训练集标签
    Test_x: 测试集
    Test_y: 测试集标签
    """
    # 定义模型名
    model_name = 'svc.model'
    # 把训练集划分成训练集和验证集用来寻参
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)
    # 定义寻参范围
    # 方式一
    # param_grid = {
    #    'C': [2 ** i for i in range(-5, 15, 2)],
    #    'gamma': [2 ** i for i in range(3, -15, -2)],
    # }
    # 方式二
    param_grid = {
         'C': [1e3, 5e3, 1e4, 5e4, 1e5],
         'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1],
    }
    # 定义模型
    svc = SVC(kernel='rbf', class_weight='balanced')
    # 定义用于网格寻参的对象，每次锁定一组参数，进行5折交叉验证，评定标准为 F1-score
    clf = GridSearchCV(svc, param_grid, scoring='f1', n_jobs=10, cv=5)
    # 开始在数据集 X_train, y_train 上进行网格寻参
    clf.fit(X_train, y_train)
    # 打印最优的参数
    print(clf.best_estimator_)
    # 评估模型，这里的原理是利用最优参数对应的模型进行预测
    y_pred = clf.predict(X_test)
    print(classification_report(y_test, y_pred))
    print(confusion_matrix(y_test, y_pred))
    # 使用最优参数组合重新定义 svc 模型
    svc = SVC(kernel='rbf', class_weight='balanced',
              C = clf.best_params_['C'], gamma=clf.best_params_['gamma'])
    # 训练模型
    model = svc.fit(X, y)
    # 打印模型信息
    print(model)
    predict = model.predict(Test_x);
    print((test_y == predict).mean())
    # 保存模型
    with open('svc.model', 'wb') as f:
        pickle.dump(model, f)
    print('save svc.model done')
    return model


if __name__ == '__main__':
    # 文件路径
    train_path = "cacheData_Feature_Vector_SVM_train.txt"
    test_path = "cacheData_Feature_Vector_SVM_test.txt"
    # 训练集和标签
    X, y = loadDataSet(train_path)
    # 测试集和标签
    test_x, test_y = loadDataSet(test_path)
    # 合并
    X.extend(test_x)
    y.extend(test_y)
    # 最大最小缩放
    scaler = MinMaxScaler()
    X = scaler.fit_transform(X)
    # 划分出训练集和测试集
    Train_x = X[0 : 1400]
    Test_x = X[1400:]
    Train_y = y[0 : 1400]
    test_y = y[1400:]
    # 验证尺寸
    print(Train_x.shape)
    print(Test_x.shape)
    # 使用两种算法
    ml_svc(Train_x, Train_y, Test_x, test_y)
    rf = RF()
    rf.train(Train_x, Train_y, Test_x, test_y)
```

## 程序运行结果

```
# 训练集大小
(1400, 12)
# 测试集大小
(42, 12)
# 以下是最优的SVC模型信息
# print(cfl.best_estimator_)
SVC(C=100000.0, cache_size=200, class_weight='balanced', coef0=0.0,
  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',
  max_iter=-1, probability=False, random_state=None, shrinking=True,
  tol=0.001, verbose=False)
#  print(classification_report(y_test, y_pred))
             precision    recall  f1-score   support

         -1       0.80      0.85      0.82       180
          1       0.88      0.84      0.86       240

avg / total       0.85      0.85      0.85       420
# 混淆矩阵
# print(confusion_matrix(y_test, y_pred))
[[153  27]
 [ 38 202]]
# print(model)
SVC(C=100000.0, cache_size=200, class_weight='balanced', coef0=0.0,
  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',
  max_iter=-1, probability=False, random_state=None, shrinking=True,
  tol=0.001, verbose=False)
# 正确率
0.809523809524
# 保存模型成功
save svc.model done
# 随机森林算法的输出
[n_estimators, acc]: 5 0.911904761905
[n_estimators, acc]: 10 0.885714285714
[n_estimators, acc]: 15 0.909523809524
[n_estimators, acc]: 20 0.9
[n_estimators, acc]: 25 0.914285714286
[n_estimators, acc]: 30 0.92380952381
[n_estimators, acc]: 35 0.921428571429
[n_estimators, acc]: 40 0.911904761905
[n_estimators, acc]: 45 0.914285714286
[n_estimators, acc]: 50 0.909523809524
[n_estimators, acc]: 55 0.909523809524
[n_estimators, acc]: 60 0.911904761905
[n_estimators, acc]: 65 0.921428571429
[n_estimators, acc]: 70 0.909523809524
[n_estimators, acc]: 75 0.911904761905
[n_estimators, acc]: 80 0.914285714286
[n_estimators, acc]: 85 0.907142857143
[n_estimators, acc]: 90 0.92619047619
[n_estimators, acc]: 95 0.916666666667
[n_estimators, acc]: 100 0.911904761905
[n_estimators, acc]: 105 0.919047619048
[n_estimators, acc]: 110 0.904761904762
[n_estimators, acc]: 115 0.904761904762
[n_estimators, acc]: 120 0.919047619048
[n_estimators, acc]: 125 0.916666666667
[n_estimators, acc]: 130 0.919047619048
[n_estimators, acc]: 135 0.914285714286
[n_estimators, acc]: 140 0.921428571429
[n_estimators, acc]: 145 0.911904761905
[n_estimators, acc]: 150 0.928571428571
[n_estimators, acc]: 155 0.914285714286
[n_estimators, acc]: 160 0.911904761905
[n_estimators, acc]: 165 0.919047619048
[n_estimators, acc]: 170 0.916666666667
[n_estimators, acc]: 175 0.909523809524
[n_estimators, acc]: 180 0.919047619048
[n_estimators, acc]: 185 0.914285714286
[n_estimators, acc]: 190 0.914285714286
[n_estimators, acc]: 195 0.902380952381
acc: 0.880952380952
```

## 数据集

暂不公开数据集，并不影响对代码的理解。

