---
title: 高斯混合模型
categories: 机器学习
tags: 机器学习
mathjax: true
date: 2018-05-15 23:52:50
---

## 准备
假设给定数据集 $\{x^{(1)}, x^{(2)}, \cdots, x^{(m)}\}$，注意没有标签。

## 简介
高斯混合模型（MoG）是一种无监督学习算法，常用于聚类。对一个样本来说，MoG 得到的是其属于各个类的概率(通过计算后验概率得到)，而不是完全的属于某个类，这种聚类方法被称为**软聚类**。一般说来，任意形状的概率分布都可以用多个高斯分布函数去近似。
在高斯混合模型中，样本属于哪个分布可以看成是一个隐含变量 $z$，则高斯混合模型有以下两个假设：
1. $z$ 服从多项式分布 $z^{(i)} \sim Multinational(\phi)$ where $\phi_j \geq 0$ and $\sum_{j=1}^k \phi_j = 1$，特别的当高斯混合模型只含两个分布时，$z$ 服从二项分布。
2. 当 $z$ 已知时（样本属于哪个分布已知），$x$ 服从正态分布，也就是说 $p(x|z)$ 服从正态分布 $p(x^{(i)} | z^{(i)}) \sim N(\mu_j,\Sigma_j)$

有了以上了个假设，我们可以计算 $x$ 与 $z$ 的联合概率分布：
$$
p(x^{(i)}, z^{(i)}) = p(x^{(i)} | z^{(i)}) p(z^{(i)})
$$
有了联合概率分布，我们写出对数似然函数：
$$
\begin{aligned}
l(\phi,\mu,\Sigma) =& \sum_{i=1}^m log p(x^{(i)};\phi,\mu,\Sigma) \\
=& \sum_{i=1}^m log \sum_{z^{(i)}=1}^k p(x^{(i)}|z^{(i)};\mu,\Sigma)p(z^{(i)};\phi)
\end{aligned}
$$
但是在高斯混合模型中每个样本对应的 $z^{(i)}$ 是未知的，也就是说每个样本属于哪个分布我们并不知道，如果假设我们已知每个样本所属的分布的话，那么接下来的问题就与高斯判别分析一样了。我们写出每个样本所属的分布已知时的对数似然函数：
$$
\begin{aligned}
l(\phi,\mu,\Sigma) =& \sum_{i=1}^m log p(x^{(i)}|z^{(i)};\mu,\Sigma) p(z^{(i)};\phi) \\
=& \sum_{i=1}^m log p(x^{(i)}|z^{(i)};\mu,\Sigma) + log p(z^{(i)};\phi)
\end{aligned}
$$

接下来的问题就简单了，对对数似然函数求导，让导数为 0，解出参数，那么混合高斯模型中所有的混合高斯分布也就知道了，聚类也就完成了。这里我们“假装”着什么都知道，来求解一下：
$$
\phi_j = \frac{1}{m} \sum_{i=1}^m I(z^{(i)} = j) \\
\mu_j = \frac{\sum_{i=1}^m I(z^{(i)}=j) x^{(i)}}{\sum_{i=1}^m I(z^{(i)} = j)} \\
\Sigma_j = \frac{\sum_{i=1}^m I(z^{(i)}=j)(x^{(i)} - \mu_j)(x^{(i)} - \mu_j)^T}{\sum_{i=1}^m I(z^{(i)} = j)} 
$$

## 使用 EM 算法解决问题
但是问题来了，我们并不知道每个样本属于哪个分布，这个时候，我们就可以使用 EM 算法了。基本步骤如下：
1. 首先初始化参数 $\phi,\mu,\Sigma$
2. 重复以下步骤直到收敛：
    （1）E-Step：对当前参数，计算 $w_j^{(i)} := p(z^{(i)}|x^{(i)}; \phi,\mu,\Sigma)$，也就是根据当前参数，计算每个样本属于各个高斯分布的概率。
    （2）M-Step：更新参数。

M-Step 中更新各个参数用到的公式如下：
$\phi_j = \frac{1}{m} \sum_{i=1}^m \omega_j^{(i)}$
$\mu_j = \frac{\sum_{i=1}^m  \omega_j^{(i)} x^{(i)}}{\sum_{i=1}^m  \omega_j^{(i)}}$
$\Sigma_j = \frac{\sum_{i=1}^m  \omega_j^{(i)}(x^{(i)} - \mu_j)(x^{(i)} - \mu_j)^T}{\sum_{i=1}^m  \omega_j^{(i)}}$

## 混合高斯模型的注意点
首先我们看一下，参数的更新公式，EM 算法并没有用到指示函数，也就说没有明确的说一个样本属于哪个类，在更新参数的过程中，只是代入了 E-Step 中计算出来的各个样本属于各个分布的概率。这使得 MoG 模型对不确定性样本处理的更好。
在 GDA 模型中，每个高斯分布使用同一个协方差矩阵，但在混合高斯模型中，各个高斯分布使用不同的协方差矩阵。

## 参考
1. 斯坦福机器学习公开课 —— 吴恩达
2. 斯坦福ML机器学习公开课笔记 —— 张雨石

