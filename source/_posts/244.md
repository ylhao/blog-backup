---
title: 集成学习（三）：随机森林
date: 2018-05-06 16:03:26
categories: 机器学习
tags: 机器学习
mathjax: true
---

## Bagging
假设给定一个包含 $m$ 个样本的数据集，首先我们先随机取出一个样本放到采样集中，然后再把这个样本放回初始的数据集中，使得下次采样时该样本仍有可能被选中，这样，经过 $m$ 次随机采样操作，我们得到含有 $m$ 个样本的采样集。初始样本集中有的样本在采样集中多次出现，有的则从未出现过。
这里我们做一个计算，假设有 $m$ 个样本，则每次随机取出一个样本的时候，每个样本被取出的概率都为 $\frac{1}{m}$，那么不被采集到的概率为：
$$
1 - \frac{1}{m}
$$
一个样本经过 m 次采样都没有被取出的概率为：
$$
(1 - \frac{1}{m}) ^ m
$$
当 $m \to \infty$ 时，有：
$$
(1 - \frac{1}{m}) ^ m \to \frac{1}{e} \approx 0.368
$$
也就是说在 bagging 每轮随机采样得到的一个采样集中，有大约 36.8% 的数据没有被采集到。这些数据常被称为袋外数据（Out of Bag, OOB），可以用这部分数据检测模型的泛化能力。

## 从方差-偏差分解的角度看 Bagging
Bagging 主要是通过集成来降低方差，比如当基学习器为不剪枝决策树时，这种说法就比较好理解了。不剪枝的决策树很容易过拟合，模型过于复杂，导致“低偏差，高方差”的情况出现，通过集成，训练多棵决策树，来降低方差，得到一个“低偏差，低方差”的模型。

## 基本结合策略
如果是解决分类问题，可以用投票法确定最终类别（少数服从多数的原则）。如果是解决回归问题，可以计算所有基学习器的预测结果的平均值或加权平均值来得到预测结果。

## 随机森林
- 随机森林的思想仍是 bagging
- 随机森林使用 CART 决策树作为基分类器
- 随机森林引入了属性扰动来增强基分类器的多样性

## 属性扰动
bagging 通过采样（样本扰动）使基分类器具有一定程度的多样性。
假设每个样本有 $d$ 个属性，随机森林在采样的基础上还会随机的选择 $k$($k < d$) 个属性（而不是用全部的 $d$ 个属性）来训练基分类器。这就进一步增强了基学习器的“多样性”，这也就是上面介绍的通过属性扰动增强基分类器的多样性。一般推荐 $k=log_2d$。

## 参考
1. 《机器学习》 —— 周志华
2. [Bagging与随机森林算法原理小结](https://www.cnblogs.com/pinard/p/6156009.html)
