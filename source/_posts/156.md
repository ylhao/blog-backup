---
title: 集成学习（二）：Adaboost
date: 2018-05-06 13:07:38
categories: 机器学习
tags: 机器学习
mathjax: true
---

Adaboost 是 Boosting 族算法的代表。这篇文章将介绍一下 Adaboost 算法。

## 预备
- 训练数据 $T = \{(x_1, y_1), (x_2, y_2),\cdots, (x_N, y_N)\}$
- 假设 $\chi$ 是实例空间
- 假设 $\gamma$ 是标记集合
- $x_i  \in \chi \subseteq R^n$
- $y_i \in \gamma = \{-1, +1\}$

## Adaboost 算法执行过程归纳
1. 初始化训练集中每个样本的权重值为 $\frac{1}{N}$
2. 迭代 M 次，每次迭代都会生成一个基学习器，首先要检查当前生成的基学习器是否满足基本条件（基学习器在训练数据集上的加权错误率是否小于 0.5）
(1)如果不满足上述基本条件，则当前基学习器被抛弃且学习过程停止。
(2)如果满足上述基本条件，那么根据当前基分类器的加权错误率调整训练数据集中样本的权值分布（增加分类错误的样本的权重，减少分类正确的样本的权重）。
3. 将所有的基学习器加权组合得到最终的分类器。

## 展开描述算法执行过程
假设算法共计迭代 $M$ 次，当前迭代次数为第 $m$ 次，对于 $m = 1,2,\cdots,M$，使用 $D_m$ 代表第 $m$ 次迭代的训练集的权值分布，使用 $G_m(x)$ 代表第 $m$ 次迭代得到的基分类器。
训练集初始的权值分布为：
$$
D_1 = (w_{11}, \cdots,w_{1i}, \cdots, w_{1N}), \quad, w_{1i} = \frac{1}{N}, \quad, i=1,2,3,\cdots,N
$$
第 $m$ 次迭代的错误率计算方式如下，其中 $w_{mi}$ 是第 $m$ 次迭代中，第 $i$ 个样本的权重，$I(\cdot)$ 为指示函数，可以看出这里计算的错误率实际上是对被分错的样本的一个**加权错误率**：
$$
e_m = P(G_m(x_i) \neq y_i) = \sum_{i=1}^N w_{mi} I(G_m(x_i) \neq y_i)
$$
第 $m$ 次迭代为 $G_m(x)$ 分配的权重为：
$$
\alpha_m = \frac{1}{2} log \frac{1-e_m}{e_m}
$$
这里我们可以发现当 $e_m = \frac{1}{2}$ 时，$\alpha_m = 0$，也就相当于这个基分类器被抛弃了，所以当发生这种情况时，当前基分类器被抛弃并且整个训练过程停止。当 $e_m < \frac{1}{2}$ 的情况与此类似。另外，$\alpha_m$ 同样是第 $m$ 个分类器的权重，描述了第 $m$ 个分类器在最终分类器中的作用有多大。
当 $e_m > \frac{1}{2}$ 时，更新第 $m+1$ 次迭代时对应的数据集的权重分布：
$$
D_{m+1} = (w_{m+1,1}, w_{m+1, w}, \cdots, w_{m+1, N})\\
w_{m+1, i} = \frac{w_{m, i}}{Z_m} exp(- \alpha_m y_i G_m(x_i))\\
Z_m = \sum_{i=1}^N w_{mi} exp (-\alpha_m y_i G_m(x_i))
$$
其中 $Z_m$ 的作用就是让 $D_{m+1}$ 所有元素的和为 1。并且容易看出来当 $G_m(x_i) \neq y_i$ 时，也就是当前基分类器分类错误的时候，$y_i G_m(x_i) = -1$，此时：
$$
w_{m+1, i} = \frac{w_{m, i}}{Z_m} exp(\alpha_m)
$$
可以发现，**被分错的样本的权重会增大**，同理可以推出被分错的样本的权重会减小。
最后，我们就可以将各个分类器进行线性组合得到最终的分类器了：
$$
f(x) = \sum_{m = 1}^M \alpha_m G_m(x)\\
G(x) = sign(f(x)) = sign \Big(\sum_{m = 1}^M \alpha_m G_m(x) \Big)
$$

## 补充
Adaboost 的训练过程是一个加权训练的过程。因为其选择当前最优基分类器时，选择的是加权错误率最低的模型。从偏差-方差分解的角度看，Boosting 主要关注降低偏差。

## 参考
1. 《机器学习》 —— 周志华
2. 《统计学习方法》 —— 李航

