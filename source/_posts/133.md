---
title: 线性判别分析
categories: 机器学习
tags: 机器学习
mathjax: true
date: 2018-05-18 21:28:26
---

在学习 LDA 之前，有必要将其自然语言处理领域的 LDA 区别开来，在自然语言处理领域， LDA 是隐含狄利克雷分布（Latent Dirichlet Allocation，简称 LDA），他是一种处理文档的主题模型。这里讨论的 LDA 是线性判别分析。

## 线性判别分析的基本原理
LDA 是一种监督学习的降维技术，也就是说它的数据集的每个样本是有类别输出的。这点和 PCA 不同。PCA 是不考虑样本类别输出的无监督降维技术。LDA 的思想可以用一句话概括，就是“**投影后类内方差最小，类间方差最大**”。更明确的含义就是我们要将数据在低维度上进行投影，投影后希望每一种类别数据的投影点尽可能的接近，而不同类别的数据的类别中心之间的距离尽可能的大。比如当我们的每个样本的特征是二维的，我们希望将二维特征降到一维，也就是说我们要找一条直线（直线方向为 $\omega$）来投影，寻找的是尽可能使样本点分离并且相同类别的样本点尽可能密集的直线。如下图所示：
![LAD算法原理演示](/img/linear_discriminant_analysis_1.png)
从直观上看，右图更好。


## 线性判别分析（二类情况）
假设我们有数据集 $D=\{(x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}), ...,(x^{(m)},y^{(m)})\}$，一共有 $m$ 个样本，其中 $y^{(i)} \in \{c_1,c_2\}$，$x^{(i)}$ 的维度为 $n$，$x^{(i)} = [x_1^{(i)},x_2^{(i)},\dots,x_n^{(i)}]^T$。另外假设我们一共有 $N_1$ 个样本属于 $c_1$ 类，这 $N_1$ 个点的集合为 $X_1$，$N_2$ 个样本属于 $c_2$ 类，这 $N_2$ 个点的集合为 $X_2$。现在我们觉得原始特征数太多，想将 $n$ 维特征降到只有一维。也就是将数据点投影到一条直线上。这条直线的基本选取规则在上文已经介绍过了。现在假设这条直线的最佳方向为 $\omega$。

样例点 $x^{(i)}$ 到直线的投影可以表示为：
$$
\omega^T x^{(i)}
$$

每类样例的均值（中心点）为：
$$
\mu_j = \frac{1}{N_j}\sum\limits_{x^{(i)} \in X_j}x^{(i)}\;\;(j=1,2)
$$

投影后的每类样本点的均值（中心点）为：
$$
\widetilde{\mu_j} = \frac{2}{N_j}\sum\limits_{x^{(i)} \in X_j} \omega^Tx^{(i)} = \omega^T \mu_j\;\;(j=1,2)
$$

首先考虑使“不同类别的类别中心之间的距离尽可能的大”。也就是使下式的 $J(\omega)$ 越大越好。
$$
J(w) = \lvert \widetilde{\mu_1} - \widetilde{\mu_2}\rvert = \lvert \omega^T (\mu_1 - \mu_2) \rvert
$$

但是只考虑这一点是不够的，如下图所示，样本点均匀分布在椭圆里，投影到横轴 $x_1$ 上时能够获得更大的中心点间距 $J(w)$，但是由于有重叠，$x_1$ 不能分离样本点。投影到纵轴 $x_2$ 上，虽然 $J(w)$ 较小，但是能够分离样本点。因此我们还需要考虑样本点之间的方差，方差越大，样本点越难以分离。
![只考虑使不同类别的类别中心之间的距离尽可能的大](/img/linear_discriminant_analysis_2.png)

我们使用另外一个度量值，称作散列值（scatter），对投影后的类求散列值：
$$
\widetilde{s_j} ^ 2 = \sum_{x^{(i)} \in X_j}(\omega x^{(i)} - \widetilde{\mu_j})^2 \;\; (j = 1,2)
$$
从公式中可以看出，只是少除以样本数量的方差值，散列值的几何意义是样本点的密集程度，值越大，越分散，反之，越集中。

而我们想要的投影后的样本点的样子是：不同类别的样本点越分开越好，同类的越聚集越好，也就是均值差越大越好，散列值越小越好。我们结合散列值可以得到下式：
$$
J(w) = \frac{\lvert \widetilde{\mu_1} - \widetilde{\mu_2}\rvert^2}{\widetilde{s_1}^2 + \widetilde{s_2}^2}
$$

将散列值展开：
$$
\begin{aligned}
\widetilde{s_j} ^ 2 
=& \sum_{x^{(i)} \in X_j}(\omega x^{(i)} - \widetilde{\mu_j})^2 \\
=& \sum_{x^{(i)} \in X_j}(\omega x^{(i)} - \omega \mu_j)^2 \\
=& \sum_{x^{(i)} \in X_j}\omega^T (x^{(i)} - \mu_j)(x^{(i)} - \mu_j)^T \omega
\end{aligned}
$$

为了简化式子，我们做以下定义：
$$
s_j = \sum_{x^{(i)} \in X_j}(x^{(i)} - \mu_j)(x^{(i)} - \mu_j)^T
$$

这个公式的样子不就是少除以样例数的协方差矩阵么，称为散列矩阵（scatter matrices）。

当确定了 $\omega$，也就确定了 $s_1$ 和 $s_2$。我们定义：
$$
s_{\omega} = s_1 + s_2
$$
其中 $S_{\omega}$ 称为 within-class scatter matrix。

我们可以得到：
$$
\widetilde{s_j}^2 = \omega^T s_j \omega \\
\widetilde{s_1}^2 + \widetilde{s_2}^2 = \omega^T s_{\omega} \omega
$$

然后我们展开分子：
$$
\lvert \widetilde{\mu_1} - \widetilde{\mu_2}\rvert ^2 = (\omega^T (
\mu_1 - \mu_2)) ^2 = \omega^T(\mu_1 - \mu_2)(\mu_1 - \mu_2)^T \omega
$$

同样为了简化式子，我们令：
$$
S_b = (\mu_1 - \mu_2)(\mu_1 - \mu_2)^T
$$
$S_b$ 称为 between-class scatter。

我们最终可以推导出我们的优化目标为：
$$
\underbrace{arg\;max}_w\;\;J(w) = \frac{w^TS_bw}{w^TS_ww} 
$$

在我们求导之前，需要对分母进行归一化，因为不做归一的话，$\omega$ 可以任意的扩大缩小。那么就没有办法确定 $\omega$ 的方向了。所以我们令 $\lVert \omega^T S_{\omega} \omega \rVert$ = 1。那么可以发现这是一个带约束条件的求最大值的问题。哦我们可以用拉格朗日乘数法来求解，我们写除拉格朗日函数：
$$
C(\omega) = \omega^T S_b \omega - \lambda(\omega^T S_{\omega} \omega)
$$
求导，令导数为 0：
$$
S_b \omega = \lambda S_{\omega} \omega
$$

若 $S_{\omega}$ 可逆，那么：
$$
S_{\omega}^{-1} S_b \omega = \lambda S_{\omega}^{-1} S_{\omega} \omega = \lambda \omega
$$
也就是说此时 $\omega$ 为 $S_{\omega}^{-1} S_b$ 的特征向量。这个公式称为 Fisher linear discrimination。

再观察下以下公式：
$$
S_b = (\mu_1 - \mu_2)(\mu_1 - \mu_2)^T
$$

两边同时乘上 $\omega$，可得：
$$
S_b \omega = (\mu_1 - \mu_2)(\mu_1 - \mu_2)^T \omega = (\mu_1 - \mu_2) \lambda_w
$$

将上式带入 $S_{\omega}^{-1} S_b \omega = \lambda S_{\omega}^{-1} S_{\omega} \omega = \lambda \omega$ 可得：
$$
S_{\omega}^{-1} (\mu_1 - \mu_2) \lambda_w = \lambda \omega
$$
再因为对 $\omega$ 扩大缩小任何倍不影响结果，因此可以约去两边的未知常数 $\lambda$ 和 $\lambda_w$，得到下式：
$$
S_{\omega}^{-1} (\mu_1 - \mu_2)= \omega
$$

至此，我们只需要求出原始样本的均值和方差就可以求出最佳的方向 $\omega$，这就是 Fisher 于 1936 年提出的线性判别分析。

## 参考
1. [线性判别分析（Linear Discriminant Analysis](http://www.cnblogs.com/jerrylead/archive/2011/04/21/2024384.html)
2. [线性判别分析LDA原理总结](https://www.cnblogs.com/pinard/p/6244265.html)

