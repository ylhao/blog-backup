---
title: 朴素贝叶斯算法
categories: 机器学习
tags: 机器学习
mathjax: true
date: 2018-05-15 23:46:53
---

## 贝叶斯公式
朴素贝叶斯算法的核心公式自然是贝叶斯公式：
$$
P(B|A) = \frac{P(A|B)P(B)}{P(A)}
$$

在机器学习分类算法中，用以下形式可能会更清晰明了：
$$
P(类别|特征) = \frac{P(特征|类别)P(类别)}{P(特征)}
$$

## 判别学习算法和生成学习算法
对于一个分类问题来说（这里以二分类问题为例），不管是感知器算法还是逻辑回归算法，都是在解空间中寻找一条直线从而把两种类别的样例分开，对于新的样例只要判断在直线的哪一侧即可，这种直接对问题求解的方法可以成为判别学习方法。

生成学习算法则是对两个类别分别进行建模，用新的样例去匹配两个模型，匹配度较高的作为新样例的类别。

## 朴素贝叶斯算法的基本思想
如果要解决的是一个分类问题，那么我们的任务是根据样本的特征来判断样本属于哪个类别。朴素贝叶斯的思想是利用训练集中的样本先计算各个类别的先验概率 $P(类别y)$ 和条件概率 $P(特征i的第k个可取值|类别y)$，然后给定测试集上的一个新样本，我们可以按照贝叶斯公式求出各个类别对应的后验概率 $P(类别y|特征)$。使得后验概率最大的类别就是这个新样本的类别。
$$
P(类别y|特征) = \frac{P(特征|类别y)P(类别y)}{P(特征)}
$$

由于对所有类别来说，$P(特征)$ 是完全相同的，我们的最终目标是比较各个类别的 $P(类别y|特征)$ 的大小，所以，为了简化计算，分母部分可以去掉。


## 朴素一词的含义
朴素贝叶斯算法假设各个特征相互独立，这样的话，对于测试集上的一个新样本来说，有以下等式成立：
$$
P(特征1, 特征2,\dots, 特征n|类别y) = P(特征1|类别y)P(特征2|类别y),\cdots,P(特征n|类别y)
$$

所以我们比较各个类别对应的：
$$
P(特征1|类别y)P(特征2|类别y)P \cdots (特征n|类别y)P(类别y)
$$
的大小即可。

## 拉普拉斯平滑
在对条件概率 $P(特征i的第k个可取值|类别y)$ 进行建模时，我们发现它们很有可能为 0，然后我们发现 $P(特征1|类别y)P(特征2|类别y)P \cdots (特征n|类别y)P(类别y)$ 中有任何一部分的值为 0，则整个式子的值为 0，为了避免这种情况，引入拉普拉斯平滑，在建模过程中，假定每个特征的每个取值至少出现 1 次，这样：
$$
P(特征i的第k个取值|类别y) = \frac{类别为y的样例中特征i的第k个取值出现的次数 + 1}{类别为y的样例数+特征i的可取值数}
$$

## 参考
1. 《统计学习方法》 —— 李航
2. 机器学习ML斯坦福公开课笔记5 —— 张雨石
3. 机器学习斯坦福公开课 —— 吴恩达
4. [理解朴素贝叶斯算法中的拉普拉斯平滑](https://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&mid=2247483832&idx=1&sn=30ee8c80552ec43b6030a5301248e1dc&chksm=ebb4396cdcc3b07ae045c9031537eb02f39fd8b2f78d36478d2c319cfb2cbad06c53ec0b9475&scene=21#wechat_redirect)
5. [带你搞懂朴素贝叶斯分类算法](https://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&mid=2247483819&idx=1&sn=7f1859c0a00248a4c658fa65f846f341&chksm=ebb4397fdcc3b06933816770b928355eb9119c4c80a1148b92a42dc3c08de5098fd6f278e61e&scene=21#wechat_redirect)

