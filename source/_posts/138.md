---
title: 线性回归（二）
categories: 机器学习
tags: 机器学习
mathjax: true
date: 2018-05-19 15:47:24
---

## 符号定义
1. $m$：训练样本的数目
2. $(x^{(i)}, y^{(i)})$：训练集中的第 i 个样本
3. 令 $x^{(i)} = [x_1^{(i)},x_2^{(i)},\dots,x_n^{(i)}] ^ T,\;\; x^{(i)} \in \mathbb{R}^n$
4. 令 $y^{(i)} \in \mathbb{R}$
5. 令 $\omega = [\omega_1,\omega_2,\dots,\omega_n] ^ T,\;\; \omega \in \mathbb{R}^n$

## 假设函数
我们令假设函数为以下形式：
$$
h(x) = \omega^T x + b \\
$$

为了推导方便，我们对 $\omega$ 和 $x^{(i)}$ 做一个扩展；
$$
x^{(i)} = [x_1^{(i)},x_2^{(i)},\dots,x_n^{(i)},1] ^ T,\;\; x^{(i)} \in \mathbb{R}^{(n+1)} \\
\omega = [\omega_1,\omega_2,\dots,\omega_n, b] ^ T,\;\; \omega_i \in \mathbb{R}^{(n+1)}
$$

那么假设函数就可以写成在“线性回归（一）”那篇文章中的形式了：
$$
h(x) = \omega^T x \\
$$

## 目标函数
$$
\begin{aligned} 
J(\omega)
=& \frac{1}{2m} \sum_{i=1}^{m} (h(x^{(i)}) - y^{(i)}) ^ 2 \\
=& \frac{1}{2m} \sum_{i=1}^{m} (\omega^T x^{(i)} - y^{(i)}) ^ 2 \\
\end{aligned}
$$

## 正规方程法推导过程
在“线性回归（一）”这篇文章中，我们写出目标函数后，直接用梯度下降法或者随机梯度下降法来优化目标函数。但是这里我们不这样做，接下来我们对目标函数进行一个改写，首先要定义一些符号，首先我们定义两个矩阵 $X$ 和 $y$,其中 $X$ 矩阵的维度为 $m \times n$，$y$ 的维度为 $m \times 1$：
![](/img/linear_regression_1.png)
$$
y = [y^{(1)},y^{(2)},\dots, y^{(n)}]^T
$$

我们可以进一步将目标函数写成以下形式：
$$
J(\omega) = (y - X \omega) ^ T (y - X \omega)
$$

我们将目标函数写成了矩阵乘法的形式，但我们的目标仍然是找到一个 $\omega$ 来最小化 $J(\omega)$，我们用矩阵表示的平方误差对 $\omega$ 进行求导：
![线性回归（二）](/img/linear_regression.png)

令上述公式等于 0，可得：
$$
\widehat{\omega} = (X^TX)^{-1} X^T y
$$

可以发现我们通过求导，可以直接估计出 $\omega$，使得 $J(\omega)$ 取得最优解。y以上公式的 $\widehat{\omega}$ 表示对 $\omega$ 的最佳估计。

## 问题
值得注意的是，上述公式中包含逆矩阵，也就是说，这个方程只在逆矩阵存在的时候使用，但是我们并不能保证 $X^T X$ 一定是可逆的。

## 参考
1. 《机器学习实战》
2. [Python3《机器学习实战》学习笔记（十一）：线性回归基础篇之预测鲍鱼年龄](https://zhuanlan.zhihu.com/p/31860100)

