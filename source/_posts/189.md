---
title: 岭回归
categories: 机器学习
tags: 机器学习
mathjax: true
date: 2018-05-20 12:04:22
---

回归分析中最常用的最小二乘法是一种无偏估计。线性回归问题的损失函数通常为：
$$
\begin{aligned}
J(\omega)
=& \frac{1}{2m} \sum_{i=1}^{m} (h(x^{(i)}) - y^{(i)}) ^ 2 \\
=& \frac{1}{2m} \sum_{i=1}^{m} (\omega^T x^{(i)} - y^{(i)}) ^ 2 \\
\end{aligned}
$$
求解以上损失函数的最小值有两种方法：**梯度下降法**以及**正规方程法**，在“线性回归（一）”和“线性回归（二）”两篇文章中分别做了相关的介绍。

## 岭回归的作用
岭回归的出现是为了解决线性回归出现的过拟合问题以及在通过正规方程方法求解 $\omega$ 的过程中出现的 $X^T X$ 不可逆这两类问题的。

岭回归的损失函数为以下形式：
$$
J(\omega) = \frac{1}{2m} \sum_{i=1}^{m} (h(x^{(i)}) - y^{(i)}) ^ 2  + \frac{\lambda}{2m} \sum_{j=1} ^ n \omega_j^2
$$

当用正规方程法来求解宿舍呢hi函数的最小值时，相当于在矩阵 $X^T X$ 上加入了一个 $\lambda I$ 从而使得矩阵非奇异。进而可以对矩阵 $X^T X + \lambda I$ 求逆。其中 $I$ 是一个单位矩阵。这时回归系数的计算公式变为：
$$
\widehat{\omega} = (X^TX + \lambda I)^{-1} X^T y
$$

## 对岭回归的理解
岭回归最先用来处理特征数多于样本数的情况，现在也用于在估计中加入偏差，从而得到更好的估计。这里通过引入 $\lambda \sum_{j=1} ^ n \omega_j^2$ 限制了所有 $\omega$ 的平方和，通过引入该惩罚项，能够减少不重要的参数。

其中 $\lambda$ 称为正则化参数，如果 $\lambda$ 选取过大，会把所有参数 $\omega$ 均最小化，造成欠拟合，如果 $\lambda$ 选取过小，会导致对过拟合问题解决不当。

这里对岭回归做更进一步的总结：岭回归是对最小二乘回归的一种补充，它损失了无偏性，来换取高的鲁棒性。

## Lasso 回归简介
岭回归与 Lasso 回归最大的区别在于岭回归引入的是 L2 范数惩罚项，Lasso 回归引入的是 L1 范数惩罚项，Lasso 回归的损失函数为以下形式：
$$
J(\omega) = \frac{1}{2m} \sum_{i=1}^{m} (h(x^{(i)}) - y^{(i)}) ^ 2  + \frac{\lambda}{2m} \sum_{j=1} ^ n \lvert \omega_j \rvert
$$

## 岭回归图示
![岭回归图示](/img/ridge_regression.png)
由上图可以看出： 当 $\alpha$ 很大时，系数变得趋近于 0，当 $\alpha$ 很小时，结果趋近于最小二乘法，系数的幅度波动很大。

## 参考
1. 《机器学习实战》

