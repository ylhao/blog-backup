---
title: 防止过拟合的方式
categories: 机器学习
tags: 机器学习
mathjax: true
date: 2018-05-21 14:01:09
---

## L1 正则项
一个含有 L1 正则项的损失（目标）函数通常为以下形式：
$$
\underset{\omega}{min} \; J(\omega,b) = \frac{1}{2m} \sum_{i=1}^m l(\widehat{y^{(i)}}, y^{(i)}) + \frac{\lambda}{2m} \sum_{j=1}^n \lVert \omega_j \rVert_1
$$

## L2 正则项
一个含有 L2 正则项的损失（目标）函数通常为以下形式：
$$
\underset{\omega}{min} \; J(\omega,b) = \frac{1}{2m} \sum_{i=1}^m l(\widehat{y^{(i)}}, y^{(i)}) + \frac{\lambda}{2m} \sum_{j=1}^n \lVert \omega_j \rVert_2^2
$$

## 神经网络中的 L2 正则项
神经网络中每一层的参数 $\omega$ 都对应一个矩阵。一个含有 L2 正则项神经网络的损失（目标）函数通常为以下的形式：
$$
\underset{\omega}{min} \; J(\omega^{[1]},b^{[1]},\omega^{[2]},b^{[2]},\dots,\omega^{[L]},b^{[L]}) = \frac{1}{2m} \sum_{i=1}^m l(\widehat{y^{(i)}}, y^{(i)}) + \frac{\lambda}{2m} \sum_{l=1}^L \lVert \omega^{[l]} \rVert_F^2
$$

其中：
$$
\lVert \omega^{[l]} \rVert_F^2  = \sum_i \sum_j (\omega_{ij}^{[l]}) ^ 2
$$

也就是说 $\lVert \omega^{[l]} \rVert_F^2$ 是神经网络中第 $l$ 层对应的权重矩阵的每个元素的平方和。

## 对正则项作用的总结
正则项的作用都是让权重趋于 0。所以也有人说这是权重衰减（Weight Deacy）。

## Dropout 正则化
Dropout 正则化也叫随机反向失活，这里用一个例子来说明 Dropout 的原理。

假设神经网络的第三层有 5 个神经元，并假设第三层原来的输出为:
![](/img/dropout_1.png)

假设 keep_prob 为 0.8，我们随机生成一个与第三层输出对应的向量（这里为了表述方便，暂且叫随机失活向量），得到的向量可能为以下形式：
![](/img/dropout_2.png)

接着我们用这个随机失活向量对第三层原来的输出进行失活处理（两个向量对应位置相乘），第三层的输出变为：
![](/img/dropout_3.png)

之前的 Dropout 正则化处理到这一步也就结束，但是现在常用的 Dropout 正则化处理一般还都会进行下一步处理，就是用第三层的输出值再都除以 keep_prob 得到最后的输出：
![](/img/dropout_4.png)

## Dropout 正则化的其它注意问题
1. 测试（验证）阶段不再使用 Dropout
2. 通常在计算机视觉领域 Dropout 用的比较多
3. 假设某一层的 keep_prob 为 0.8，代表着这一层的每个神经元有 20% 的概率失活，而不是说这一层一定有 20% 的神经元失活。

## 其它的防止过拟合的方法
1. 扩大训练集
2. 早停

## 参考
1. 深度学习工程师微专业 —— 吴恩达

