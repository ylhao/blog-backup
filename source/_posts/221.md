---
title: 学习率衰减
categories: 机器学习
tags: 机器学习
mathjax: true
date: 2018-05-21 19:31:36
---

### 学习率衰减（learning rate decay）的含义

在神经网络的训练过程中，减小learning rate $\alpha$ 也能有效提高神经网络训练速度。通常在训练过程中，应当使 learning rate 随着迭代次数的增加而逐渐的变小。

### 随着迭代次数的增加为什么要减小学习率？

![](/img/learning_rate_decay1.png)

如上图所示，图中蓝色的线表示使用恒定的 learning rate $\alpha$，由于每次迭代 $\alpha$ 都相同，所以步进长度不变，每次都会在接近最优值处会在一个较大的范围内振荡。图中 绿色的线表示的是使用逐渐减小的 learning rate $\alpha$。随着迭代次数增加，$\alpha$ 逐渐减小，步进长度也逐渐减小。每次能够在最优值处较小范围内微弱振荡，不断逼近最优值。

### 减小 learning rate 的方式

$$
\alpha = \frac{1}{1 + DecayRate \times epoch} \alpha_0
$$

其中：
- DecayRate： 是一个超参数。
- epoch：训练完所有样本的次数。

除了以上方式，$\alpha$ 还有以下计算方式：
- $\alpha = 0.95^{epoch} \cdot \alpha_0$
- $\alpha = \frac{k}{\sqrt{epoch}} \cdot \alpha_0$
- $\alpha = \frac{k}{\sqrt{t}} \cdot \alpha_0$

其中 k 为一个超参数，t 为 mini-batch number。

## 参考
1. 深度学习工程师 —— 吴恩达
2. [吴恩达《优化深度神经网络》精炼笔记（2）-- 优化算法](https://mp.weixin.qq.com/s?__biz=MzIwOTc2MTUyMg==&mid=2247483958&idx=1&sn=78ed60b5035ef77ad07d1b2c5e61da57&chksm=976fa7aba0182ebd5271b00dfe78f74c1be88ce286ec59c3d79cbb68b10f10051b36dbb727ec&scene=21#wechat_redirect)

