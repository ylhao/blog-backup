---
title: 神经网络参数的初始化问题
categories: 机器学习
tags: 机器学习
mathjax: true
date: 2018-05-21 11:14:53
---

## 为什么参数 $\omega$ 要随机初始化而不能全部初始化为一个相同的值？

如果将 $\omega$ 全都设置成一个相同的值，会出现“完全对称”的情况。下面举例说明“完全对称”是什么意思。

![神经网络示意图](/img/neural_network_1.png)
如上图所示，当隐藏层的 4 个神经元对应的 $\omega$ 全部初始化为一个相同值时，这 4 个神经元除了偏置可能互不相同外，其它相关因素都是相同的（比如：输入、参数等）。这会导致这 4 个神经元的输出也基本上是相同的，进而导致下一层也是类似的情况，也就是说这会导致每层的神经元基本上都是相同的。起到的作用跟用一个神经元差不多，导致模型的表达能力大大下降。由于我们需要的是尽可能不同的神经元，所以我们需要随机初始化 $\omega$ 来增加神经元的多样性。

## 为什么初始值要设置的很小？
将初始值设置的很小是考虑到激活函数的一些特性，在 sigmoid 函数和 tanh 函数中，我们在“对常见的激活函数的总结”一文中提到过当 $\lvert z \rvert$ 很大时，激活函数的斜率（梯度）很小，因此，在这个区域内，梯度下降算法会运行得比较慢。在实际应用中，应该尽量避免使 $\lvert z \rvert$ 落在这个区域，应该使 $\lvert z \rvert$ 尽可能的限定在 0 值附近，提高梯度下降法的效率。我们将初始值设置的很小，就是为了这个目的。

## 初始化 $\omega$ 时可供参考的经验
下面以 numpy 为例，介绍下当选取不同的激活函数时，应该如何随机初始化参数 $\omega$。

当选的激活函数为 ReLU 时，可按照以下方式初始化各层的权重矩阵：
$$
w^{[l]} = np.random.randn(shape) * np.sqrt(\frac{2}{n^{[l-1]}})
$$

当选的激活函数为 tanh 时，可按照以下方式初始化各层的权重矩阵：
$$
w^{[l]} = np.random.randn(shape) * np.sqrt(\frac{1}{n^{[l-1]}})
$$

或者按以下方式：
$$
w^{[l]} = np.random.randn(shape) * np.sqrt(\frac{2}{n^{[l-1]} + n^{[l]}})
$$

其中 np.random.randn(shape) 得到的是一个形状为 shape 的矩阵，矩阵中的每个元素都符合标准正态分布（$\mu = 0, \; \sigma^2 = 1$）。

## 参考
1. 深度学习工程师微专业 —— 吴恩达

