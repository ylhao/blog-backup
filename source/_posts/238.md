---
title: 感知机（三）
date: 2018-05-04 13:26:33
categories: 机器学习
tags: 机器学习
mathjax: true
---

这篇文章主要讨论感知算法的对偶形式。

之前的文章，我们得到如果使用随机梯度下降法，那么更新 $w, b$ 的式子如下：
$$
w \leftarrow w + \eta y_ix_i \\
b \leftarrow b + \eta y_i\\
$$
假设对于一个实例 $(x_i, y_i)$，一共执行了 $n_i$ 次更新过程，那么 $w, b$ 关于 $(x_i, y_i)$ 的增量分别为 $n_i \eta y_i x_i, n_i \eta y_i$。
令：
$$
\alpha_i = n_i \eta
$$
同时设一共有 N 个点，那么可以得到下式：
$$
w = \sum_{i=1}^{N}\alpha_i y_i x_i \\
b = \sum_{i=1}^{N}\alpha_i y_i \\
\alpha_i \geq 0
$$
特别的，当 $\eta = 1$ 时，$\alpha_i$ 代表由于第 $i$ 个实例点误分而进行更新的次数。

## 引出对偶形式
将上面得到的 $w$ 代到原来的感知机模型中，得到原感知机模型的对偶形式：
$$
f(x) = sign\Big(\sum_{j=1}^{N}\alpha_j y_j x_j \cdot x + b \Big)
$$
我们可以认为模型的参数为 $\alpha$ 和 $b$，其中：
$$
\alpha = (\alpha_1, \alpha_2, \dots ,\alpha_N)
$$
在训练集中选一个点 $(x_i, y_i)$，当该点分类错误时，有：
$$
yi\Big(\sum_{j=1}^{N}\alpha_j y_j x_j \cdot x + b \Big) < 0
$$
此时，需要更新 (x_i, y_i) 对应的参数：
$$
\alpha_i = \alpha_i + \eta \\
b = b + \eta y_i
$$
然后再选选练集的下一个点，直到所有的点都被分对。其中式子 $\alpha_i = \alpha_i + \eta$ 可以由式子 $\alpha_i = n_i \eta$ 推出。

## Gram 矩阵
我们可以将所有的实例的内积计算出来，存到矩阵中，这个矩阵就是 Gram 矩阵，$G = [x_i \cdot x_j]_{N \times N}$。
例如，我们假设 $x_1=(3, 3)^T, x_2 = (4, 3)^T, x_3=(1,1)^T$，计算得到的 Gram 矩阵为：
$$
G = 
\left[
\begin{array}{ccc}
18&21&6\\
21&25&7\\
6&7&2
\end{array}
\right]
$$
计算这个矩阵以后，我们就可以直接查矩阵中的元素去获取对应的向量内积了。最后总结感知机的对偶形式也是可以收敛的，与原始形式一样，存在多个解。

## 参考
- 《统计学习方法》—— 李航

