---
title: 文本预处理方式自查文档
categories: 机器学习
tags: 机器学习
mathjax: true
date: 2018-05-17 00:11:15
---

## 处理流程
通常在解决 NLP 问题的时候，需要对文本进行预处理。在这里给出几个函数供大家参考。处理流程基本分为两步：
1. 分词
2. 依据过滤规则进行过滤

## 考虑因素
- 是否要过滤停用词
- 是否要过滤标点
- 是否要过滤数字
- 大写的英文字母是否要转成小写
- 繁体字是否要转为简体字
- 是否只需要保留汉字
- 是否需要过滤某一词性的词

## 参考代码
```
# encoding:utf-8

import re
import jieba
from multiprocessing import cpu_count
import platform


# 设置停用词
def set_stop_words(filepath=None):
    with open(filepath, 'r', encoding='utf-8') as f:
        stop_words = set([line.strip() for line in f.readlines() if line.strip() != ''])
    return stop_words


# 分词
def cut(input_filepath=None, output_filepath=None, sep='\t', split_num=1, skip_header=True):
    if platform.system() != 'Windows':
        jieba.enable_parallel(cpu_count())  # 使用所有 cpu
    fout = open(output_filepath, 'w', encoding='utf-8')
    with open(input_filepath, 'r', encoding='utf-8') as f:
        if skip_header is True:  # 直接写出 header 行
            fout.write(f.readline())
        for line in f:
            label, content = line.split(sep, split_num)
            words = ' '.join(list(jieba.cut(content)))
            fout.write('{}{}{}'.format(label, sep, words))
    fout.close()


# 过滤词
def clean(input_filepath=None, output_filepath=None, sep='\t',
          split_num=1, skip_header=True, filter_stop_word=False, stop_words=None):
    patterns = [re.compile(u'[^\u4E00-\u9FA5A-Za-z]'),  # 匹配汉字和英文
                re.compile('\s'),  # 匹配所有的空格字符包括 [\t\n\f\r\p{Z}]
                re.compile(' {2,}')]  # 匹配 2 个以上空格（包含 2 个）
    fout = open(output_filepath, 'w', encoding='utf-8')
    with open(input_filepath, 'r', encoding='utf-8') as f:
        if skip_header is True:  # 直接写出 header 行
            fout.write(f.readline())
        for line in f:
            label, content = line.split(sep, split_num)
            for pattern in patterns:  # 依据正则表达式过滤
                content = re.sub(pattern, ' ', content)
            content = content.lower() # 英文大写转小写
            content = content.strip() # 过滤掉开头和结尾的空白符
            if filter_stop_word is True:  # 判断是否需要过滤停用词
                words = [word for word in content.split(' ') if word not in stop_words]
                fout.write('{}{}{}\n'.format(label, sep, ' '.join(words)))
            else:
                fout.write('{}{}{}\n'.format(label, sep, content))
        fout.close()


if __name__ == '__main__':
    stop_words = set_stop_words('./stop_words.txt')
    cut(input_filepath='./test_data.csv', output_filepath='./test_words.csv', sep=',', split_num=1, skip_header=True)
    clean(input_filepath='./test_words.csv', output_filepath='./test_words_clean.csv', sep=',',
          split_num=1, skip_header=True, filter_stop_word=True, stop_words=stop_words)

```

## 停用词文件 stop_words.txt
```
称
上下
左右
将
由
不
与
的
在
和
是
就
有
仍
对
```

## 原文件 test_data.csv
```
id,title
1,美国副总统彭斯：（就朝鲜问题）为所有可能的结果做好准备将对任何核武器的使用进行快速应对
2,香港财政司长陈茂波：需要继续留意全球货币环境和地缘政治的变化与政策风险。
3,日本央行理事雨宫正佳：退出宽松的细节将由经济和物价状况决定。
4,日本央行新任副行长：日本不存在通缩，但是距离2%的通胀目标仍有距离。
5,德国地学研究中心：智利北部海岸附近发生6级地震。
6,美国财长努钦：美国总统特朗普与朝鲜最高领导人金正恩会面的条件是朝鲜无核化以及不再进行导弹测试。
7,据韩联社：韩国总统文在寅与美国总统特朗普通电，就与朝鲜可能的对话进行讨论。
8,美联储主席鲍威尔：预计通胀将于中期稳定在2%左右。
```

## 分词结果 test_words.csv
```
id,title
1,美国 副 总统 彭斯 ： （ 就 朝鲜 问题 ） 为 所有 可能 的 结果 做好 准备 将 对 任何 核武器 的 使用 进行 快速 应对 
2,香港 财政 司长 陈茂波 ： 需要 继续 留意 全球 货币 环境 和 地缘 政治 的 变化 与 政策 风险 。 
3,日本央行 理事 雨宫 正佳 ： 退出 宽松 的 细节 将 由 经济 和 物价 状况 决定 。 
4,日本央行 新任 副行长 ： 日本 不 存在 通缩 ， 但是 距离 2% 的 通胀 目标 仍 有 距离 。 
5,德国 地学 研究 中心 ： 智利 北部 海岸 附近 发生 6 级 地震 。 
6,美国 财长 努钦 ： 美国 总统 特朗普 与 朝鲜 最高 领导人 金正恩 会面 的 条件 是 朝鲜 无核化 以及 不再 进行 导弹 测试 。 
7,据 韩联社 ： 韩国 总统 文在寅 与 美国 总统 特朗普 通电 ， 就 与 朝鲜 可能 的 对话 进行 讨论 。 
8,美联储 主席 鲍威尔 ： 预计 通胀 将 于 中期 稳定 在 2% 左右 。
```

## 过滤结果 test_words_clean.csv
```
id,title
1,美国 副 总统 彭斯 朝鲜 问题 为 所有 可能 结果 做好 准备 任何 核武器 使用 进行 快速 应对
2,香港 财政 司长 陈茂波 需要 继续 留意 全球 货币 环境 地缘 政治 变化 政策 风险
3,日本央行 理事 雨宫 正佳 退出 宽松 细节 经济 物价 状况 决定
4,日本央行 新任 副行长 日本 存在 通缩 但是 距离 通胀 目标 距离
5,德国 地学 研究 中心 智利 北部 海岸 附近 发生 级 地震
6,美国 财长 努钦 美国 总统 特朗普 朝鲜 最高 领导人 金正恩 会面 条件 朝鲜 无核化 以及 不再 进行 导弹 测试
7,据 韩联社 韩国 总统 文在寅 美国 总统 特朗普 通电 朝鲜 可能 对话 进行 讨论
8,美联储 主席 鲍威尔 预计 通胀 于 中期 稳定
```

