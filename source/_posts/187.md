---
title: 感知机（一）
date: 2018-05-04 09:46:26
categories: 机器学习
tags: 机器学习
mathjax: true
---

## 定义
- 输入空间 $\chi \subseteq R^n$
- 输出空间 $\gamma = \{+1,-1\}$
- 实例的特征向量 $x\in \chi $
- 每个实例对应的输出 $y \in \gamma $
- 由输入空间到输出空间的函数 $f(x) = sign(wx + b)$
- 模型参数为 $w, b$

## 符号函数
$$
sign(x)=\begin{cases}
+1,\quad x\geq 0 \\
-1,\quad x<0
\end{cases}
$$

## 学习策略
首先假设数据是线性可分的，那么 $wx + b = 0$ 对应一个分离超平面，这个超平面可以将数据分离开，
我们的目的就是让模型学习到这个分离超平面。

## 损失函数
第一个选择是用误分类点的数目构造损失函数，但是得到的损失函数不是连续可导的，不宜优化。另一个选择是误分类点到超平面的总距离。  
一个误分类点到超平面的距离为：
$$
\frac{1}{\lVert w \rVert} \lvert wx + b \rvert
$$
设误分类点为 $(x_{i}, y_{i})$，其中 $y_{i} = \pm 1$，所以误分类点到超平面的距离可以写成：
$$
-\frac{1}{\lVert w \rVert} y_{i}(wx_{i} + b)
$$
设 $M$ 为所有误分类的点，那么所有误分类的点到超平面的总距离为：
$$
-\frac{1}{\lVert w \rVert} \sum_{x_i \in M} y_{i}(wx_{i} + b)
$$
综合以上推导过程，我们设置损失函数为：
$$
L(w, b) = - \sum_{x_i \in M} y_{i}(wx_{i} + b)
$$
我们的目标是让损失函数的值尽量的小（也就是让误分类点到超平面的总距离尽量的小），基于这个目标我们得到一个最优化问题：
$$
min_{w,b} L(w, b) = - \sum_{x_i \in M} y_{i}(wx_{i} + b)
$$

## 梯度下降和随机梯度下降
首先我们需要任意选择一个超平面（随机初始化 $w$ 和 $b$），然后用梯度下降法或者随机梯度下降法不断的极小化目标函数，损失函数 $L$ 的梯度为：
$$
\nabla_w L(w,b) = - \sum_{x_i \in M}y_ix_i \\
\nabla_w L(w,b) = - \sum_{x_i \in M}y_i
$$
如果我们采用随机梯度下降，每次随机选取一个误分类点，对 $w, b$ 的更新方式为（$\eta$ 为学习率，取值范围为 $0 < \eta \leq 1$）：
$$
w \leftarrow w + \eta y_ix_i \\
b \leftarrow b + \eta y_i
$$
如果我们采用的是梯度下降法，设每次误分类点的数目为 $\lvert M \rvert$，那么对 $w, b$ 的更新方式为：
$$
w \leftarrow w + \frac{1}{\lvert M \rvert}\sum_{x_i \in M}y_ix_i \\
b \leftarrow b + \frac{1}{\lvert M \rvert}\sum_{x_i \in M}y_i
$$

所以整个感知机的训练过程就是，首先选取初始值 $w_0, b_0$，之后利用训练数据，使用梯度下降法或者随机梯度下降法，不断的更新 $w, b$，极小化目标函数 $L$，直到没有误分类点(我们假设的前提条件是线性可分)。

## 参考
- 《统计学习方法》—— 李航
