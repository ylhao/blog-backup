---
title: 特征值分解和奇异值分解
categories: 机器学习
tags: 机器学习
mathjax: true
date: 2018-05-17 19:50:56
---

## 特征值和特征向量
首先了解一下特征值和特征向量，设 $A$ 是一个 $n \times n$ 的矩阵，$x$ 是一个 $n$ 维向量，那么我们就可以说 $\lambda$ 是 $A$ 的一个特征值，$x$ 是 $A$ 的特征值 $\lambda$ 对应的特征向量。
$$
Ax = \lambda x
$$

## 特征值分解
求出特征值特征向量以后，我们就可以将方针 $A$ 进行特征分解了。假设我们求出了矩阵 $A$ 的 $n$ 个特征值 $\lambda_1 \leq \lambda_2 \leq ... \leq \lambda_n$，这 $n$ 个特征值对应的特征向量为 $w_1,w_2,\dots,w_n$。如果我们令：
$$
W = [w_1,w_2,\dots,w_n] \\
\Sigma = diag(\lambda_1, \lambda_2, \dots, \lambda_n)
$$

那么我们可以得到下式：
$$
A=W\Sigma W^{-1}
$$

最后我们做个总结，只有方阵能够进行特征分解，在特征分解中，特征值表示的是这个特征有多重要，而特征向量表示的是这个特征是什么。

## 奇异值分解

### 奇异值分解的定义
在现实世界中，并不是每个矩阵都是方阵，但是不是方阵的普通矩阵也是可以分解的。奇异值分解是一个适用于任意矩阵的分解方法。我们定义矩阵 $A$ 对应的奇异值分解为：

$$
A = U \Sigma V^T
$$

其中 $A$ 是一个 $m \times n$ 的矩阵，$U$ 是一个 $m \times m$ 的矩阵（称为左奇异矩阵），$\Sigma$ 是一个 $m \times n$ 的矩阵（除了对角线的元素都是 0，对角线上的元素称为奇异值），$V$ 是一个 $n \times n$ 的矩阵（称为右奇异矩阵）。

### 如何得到 $U$、$\Sigma$、$V^T$

#### 计算 $V$
- 计算 $A^T A$ 得到一个 $n \times n$ 的方阵。
- 求 $A^T A$ 的特征值 $\lambda_1 \leq \lambda_2 \leq ... \leq \lambda_n$ 和对应的特征向量 $v_1, v_2, \dots,v_n$。这里补充一点，因为 $A^T A$ 是一个对称阵，所以一定能够找到 $n$ 个相互正交的特征向量。
- 则先令 $V = [v_1, v_2, \dots, v_n]$，然后我们再将 $V$ 中的 $n$ 个特征向量标准化，即让 $\lVert v_i \rVert = 1$，也就是让 $v_i^T v_i = 1$，即让 $V^T V = I$。

#### 计算 $U$
- 计算 $A A^T$ 得到一个 $m \times m$ 的方阵。
- 求 $A A^T$ 的特征值 $\alpha_1 \leq \alpha_2 \leq ... \leq \alpha_m$ 和对应的特征向量 $u_1, u_2, \dots,u_m$。
- 则 先令 $U = [u_1, u_2, \dots, u_m]$。然后我们再将 $U$ 中的 $m$ 个特征向量标准化，即让 $\lVert u_i \rVert = 1$，也就是让 $u_i^T u_i = 1$，即让 $U^T U = I$。

#### 计算 $\Sigma$
$$
A = U \Sigma V^T \\
AV = U \Sigma V^T V \\
AV = U \Sigma \\
Av_i = \sigma_i u_i \\ 
\sigma_i = \frac{A v_i}{u_i}
$$
我们可以按照上面的推导过程求出所有的 $\sigma_i$，进而也就求出了 $\Sigma$。

### 再次思考 PCA
PCA 算法中，我们得到一个类似的协方差矩阵 $\Sigma$，我们求其特征值和特征向量，然后挑选一定数量的大的特征值对应的特征向量作为列向量构成矩阵，然后再对这个矩阵进行转置得到最后的矩阵，我们就可以用这个矩阵对原始数据进行降维了。比如最后的矩阵是 $k \times n$ 的，原始数据是 $n$ 维，用该矩阵就可以将原始数据降到 $k$ 维。
其实这个“最后的矩阵”就是 SVD 的右奇异矩阵。所以 PCA 算法可以用 SVD 来实现。

## 参考
1. 斯坦福机器学习公开课 —— 吴恩达
2. [奇异值分解(SVD)原理与在降维中的应用](https://www.cnblogs.com/pinard/p/6251584.html) —— 刘建平

