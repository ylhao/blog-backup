---
title: GPU 状态监测 nvidia-smi 命令详解
categories: 机器学习
tags: 机器学习
mathjax: true
date: 2018-09-06 00:10:13
---

在进行深度学习实验时，GPU 的实时状态监测十分有必要。

## nvidia-smi

![nvidia-smi](/img/nvidia-smi.png)

上图是服务器上 GeForce GTX 1080 Ti 的信息，下面对各个字段（参数）的含义进行详细介绍。

- GPU: GPU 编号
- Fan: 风扇转速（0% ~ 100%）
- Name: GPU 型号
- Temp: 温度，单位是摄氏度
- Perf: 性能状态，从 P0 到 P12（GPU 未工作时为 P0，达到最大工作限度时为 P12）
- Persistence-M: 持续模式的状态。持续模式虽然耗能大，但是在新的 GPU 应用启动时，花费的时间更少，这里显示的是 off 的状态
- Pwr:Usage/Cap: 能耗 
- Bus-Id: 涉及 GPU 总线的东西，domain:bus:device.function
- Disp.A: Display Active，表示 GPU 的显示是否初始化
- Memory Usage: 显存使用率
- Volatile GPU-Util: 浮动的 GPU 利用率
- Uncorr. ECC: Error Correcting Code，错误检查与纠正
- Compute M: compute mode，计算模式
- 下方的 Processes 表示每个进程对 GPU 的显存使用率

## nvidia-smi -L

该命令用于列出所有可用的 NVIDIA 设备信息。

![nvidia-smi -L](/img/nvidia-smi-l.png)

## 参考

- [GPU状态监测 nvidia-smi 命令详解](https://blog.csdn.net/huangfei711/article/details/79230446)

