---
title: 对常见的优化算法的总结
categories: 机器学习
tags: 机器学习
mathjax: true
date: 2018-05-19 15:55:32
---

这篇文章会依次介绍以下几种优化算法：
1. Momentum
2. RMSProp
3. Adam

## Gradient Descent with Momentum
首先介绍动量梯度下降法（Gradient Descent with Momentum）。基本上动量梯度下降法的优化速度要快于传统的梯度下降法。其基本思想就是在每次训练时，对参数（权重 $\omega$ 和 偏置 $b$）的梯度进行指数加权平均，然后用得到的梯度值更新参数。

![Gradient Descent with Momentum](/img/momentum.png)

原始的梯度下降算法如上图蓝色折线所示。在梯度下降过程中，梯度下降的振荡较大，此时每一点处的梯度只与当前方向有关，产生类似折线的效果，前进缓慢。而如果对梯度进行指数加权平均，这样使当前梯度不仅与当前方向有关，还与之前的方向有关，这样处理让梯度前进方向更加平滑，减少振荡，能够更快地到达最小值处。

Momentum 算法每次迭代的参数更新过程如下：
1. 计算 $d_{\omega}$ 和 $d_b$。
2. 计算 $d_{\omega}$ 和 $d_b$ 的指数加权平均。
（1）$V_{d_{\omega}} = \beta \cdot V_{d_{\omega}} + (1-\beta)d_{\omega}$
（2）$V_{d_{b}} = \beta \cdot V_{d_{b}} + (1-\beta)d_{b}$
3. 使用以下方式更新参数。
（1）$\omega = \omega - \alpha V_{d_w}$
（2）$b = b - \alpha V_{d_b}$

初始时，一般设置 $V_{d_{b}} = 0, \; V_{d_{\omega}} = 0$，通常设置 $\beta = 0.9$，另外，关于偏移校正，可以不使用。

这里根据上图对 Gradient Descent with Momentum 算法做一个更形象化的解释，我们可以发现梯度下降法在垂直方向上摆动剧烈，而 Momentum 算法计算的是梯度的指数加权平均，既然是”平均“，那就会在一定程度上减轻梯度下降法在垂直方向上剧烈摆动的情况，所以我们可以观察到 Momentum 算法对应的红线在垂直方向上摆动就不那么剧烈了。能够用更少的步数更快的到达最小值处。

## RMSprop
RMSprop 是另外一种优化梯度下降速度的算法。

RMSprop 算法每次迭代的参数更新过程如下：
1. 计算 $d_{\omega}$ 和 $d_b$。
2. 计算 $S_{d_{\omega}}$ 和 $S_{d_b}$。
（1）$S_{d_{\omega}} = \beta \cdot S_{d_{\omega}} + (1-\beta)d^2_{\omega}$
（2）$S_{d_{b}} = \beta \cdot S_{d_{b}} + (1-\beta)d^2_{b}$
3. 使用以下方式更新参数。
（1）$\omega = \omega - \alpha \frac{d_{\omega}} {\sqrt{S_{d_{\omega}}} + \varepsilon}$
（2）$b = b - \alpha \frac{d_b}{\sqrt{S_{d_{b}}} + \varepsilon}$

我们以下图为例简单的介绍一下 RMSProp 算法的思想。为了便于叙述和理解，我们假设神经网络中只有两个参数 $\omega$ 和 $b$，并假设水平方向代表参数 $w$ 的方向而垂直方向代表参数 $b$ 的方向，从图中我们可以看出，使用梯度下降法时，在垂直方向上振荡较大，也就是 $d_b$ 比较大，而在水平方向上振荡较小，也就是 $d_{\omega}$ 较小。因此在上述过程中 $S_{d_b}$ 较大，而 $S_{d_{\omega}}$ 较小。在更新参数的过程中，我们将  $S_{d_b}$ 和  $S_{d_{\omega}}$ 放在分母的位置上，这样就使得在垂直方向上的振荡程度相对于在水平方向的变化程度要小了一些。比如说原来在水平方向上前进了 3 步的距离，在垂直方向上了 10 次，现在可能就是在水平方向上前进了 3 步的距离，在垂直方向上却只振荡了 6 次。

![RMSprop](/img/rmsprop.png)

用一句话概括就是，在要消除摆动的维度中，通常梯度是大的，我们通过以上更新参数的方式来消除摆动。

## Adam
Adam 算法每次迭代的参数更新过程如下：
1. 计算 $d_{\omega}$ 和 $d_b$。
2. 计算 $d_{\omega}$ 和 $d_b$ 的指数加权平均。
（1）$V_{d_{\omega}} = \beta_1 \cdot V_{d_{\omega}} + (1-\beta_1)d_{\omega}$
（2）$V_{d_{b}} = \beta_1 \cdot V_{d_{b}} + (1-\beta_1)d_{b}$ 
3. 计算 $S_{d_{\omega}}$ 和 $S_{d_b}$。
（1）$S_{d_{\omega}} = \beta_2 \cdot S_{d_{\omega}} + (1-\beta_2)d^2_{\omega}$
（2）$S_{d_{b}} = \beta_2 \cdot S_{d_{b}} + (1-\beta_2)d^2_{b}$
4. 进行偏移校正。
（1）$V_{d_{\omega}}^{correct} = \frac{V_{d_{\omega}}} {1-\beta_2^2}$
（2）$V_{d_{b}}^{correct} = \frac{V_{d_{b}}} {1-\beta_2^2}$
（3）$S_{d_{\omega}}^{correct} = \frac{S_{d_{\omega}}} {1-\beta_2^2}$
（4）$S_{d_{b}}^{correct} = \frac{S_{d_{b}}} {1-\beta_2^2}$
3. 使用以下方式更新参数。
（1）$\omega = \omega - \alpha \frac{V_{d_{\omega}}^{correct}} {\sqrt{S_{d_{\omega}}^{correct}} + \varepsilon}$
（2）$b = b - \alpha \frac{V_{d_b}^{correct}}{\sqrt{S_{d_{b}}^{correct}} + \varepsilon}$

实际上 Adam 算法就是 Momentum 和 RMSprop 两种算法的结合，Adam 算法结合了动量梯度下降和 RMSprop 各自的优点，使得神经网络训练速度大大提高。

## 超参推荐
1. $\beta_1$：$0.9$
2. $\beta_2$：$0.999$
3. $\varepsilon$：$10^{-8}$

## 参考
1. 深度学习工程师微专业 —— 吴恩达
2. [吴恩达《优化深度神经网络》精炼笔记（2）-- 优化算法](https://mp.weixin.qq.com/s?__biz=MzIwOTc2MTUyMg==&mid=2247483958&idx=1&sn=78ed60b5035ef77ad07d1b2c5e61da57&chksm=976fa7aba0182ebd5271b00dfe78f74c1be88ce286ec59c3d79cbb68b10f10051b36dbb727ec&scene=21#wechat_redirect)

