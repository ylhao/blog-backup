---
title: 逻辑回归
categories: 机器学习
tags: 机器学习
date: 2018-05-13 11:27:46
mathjax: true
---

假设要解决的问题是一个二分类问题，目标值为 $\{0, 1\}$，以线性回归为基础，将模型输出映射到 $[0, 1]$ 之间。我们选择这样一个函数：
$$
h_\theta(x) = g(\theta^T x) = \frac{1}{1+e^{-\theta^Tx}} \\
g(z) = \frac{1}{1 + e^{-z}}
$$
其中 $g(z)$ 被称为 sigmoid 函数。为什么选择这个函数是可以通过广义线性模型推出来的，等总结到那里的时候再具体介绍。有了这个函数，对于一个样例，就可以得到它属于各个分类的的概率值了：
$$
p(y=1|x;\theta) = h_\theta(x) \\
p(y=0|x;\theta) = 1 - h_\theta(x)
$$
我们将上面两个式子合并成一个：
$$
p(y=x|;\theta) = (h_\theta(x))^y(1-h_\theta(x))^{(1-y)}
$$
有了上面这个式子，我们就能很容易的得到函数 $h$ 在整个数据集上的似然函数：
$$
\begin{aligned}
l(\theta) =& P(Y|X;\theta) \\
=& \prod_i p(y^{(i)} | x^{(i)} ; \theta) \\
=& \prod_i (h_\theta(x^{(i)}))^{y^{(i)}} (1 - h_\theta(x^{(i)})) ^ {(1-y^{(i)})}
\end{aligned}
$$
转为对数似然函数：
$$
\begin{aligned}
L(\theta) =& logl(\theta) \\
=& \sum_{i=1}^m[y^{(i)}logh_\theta(x^{(i)})+(1-y^{(i)})log(l-h_\theta(x^{(i)}))]
\end{aligned}
$$
假设我们用**随机梯度下降法**更新参数，每次只用一个样例，则上面的对数似然函数退化成：
$$
L(\theta) = y^{(i)}logh_\theta(x^{(i)})+(1-y^{(i)})log(l-h_\theta(x^{(i)}))
$$
更新参数的公式为：
$$
\theta_j := \theta_j \alpha \frac{\partial}{\partial \theta_j} L(\theta)
$$
这个时候需要求出 $L(\theta)$ 对各个参数的偏导，如果是准备面试，请直接记住求导结果：
$$
\frac{\partial}{\partial \theta_j} L(\theta) = x_j(y - h_\theta(x))
$$
另外要记住 sigmoid 函数的求导结果：
$$
{g}'(z)=g(z)(1 - g(z))
$$
所以，参数更新公式为：
$$
\theta_j := \theta_j + \alpha (y^{(i)} - h_{\theta_j}(x^{(i)})) x^{(i)}_j
$$
参数更新公式与最小二乘的形式一样，但是实际上是不一样的，因为函数 h 不一样。但这并不是巧合，这几乎是一种通用的规则，你可以选择不同的假设，但如果使用梯度下降算法的话，更新规则都是如公式 18 的形式。
如果我们用**梯度下降法**，每次更新参数用所有样例，则参数更新公式为：
$$
\theta_j := \theta_j + \sum_{i=1}^m \alpha (y^{(i)} - h_{\theta_j}(x^{(i)})) x^{(i)}_j
$$

## 参考
1. 斯坦福机器学习课程 —— 吴恩达
2. 斯坦福ML公开课笔记 —— 张雨石

