---
title: 局部加权线性回归
categories: 机器学习
tags: 机器学习
mathjax: true
date: 2018-05-13 10:21:46
---

## 思想
局部加权回归算法是对线性回归的扩展，当目标假设是线性模型时，使用线性回归自然能拟合的很好，但如果目标假设不是线性模型，比如一个忽上忽下的函数，这时用线性模型就拟合的很差。为了解决这个问题，当我们在预测一个点的值时，我们选择和这个点相近的点而不是全部的点做线性回归。


## 目标函数
$$
J(\theta) = \frac{1}{2} \sum_{i=1}^{m} w^{(i)} (h_{\theta}(x^{(i)})-y^{(i)}) ^ 2
$$

## $w^{(i)}$ 的计算
$w^{(i)}$ 是权值，计算方式如下：
$$
w^{(i)} = exp \Big(-\frac{(x^{(i)}-x)^2}{2\tau^2} \Big)
$$
其中分子部分的 $(x^{(i)}-x)^2$ 度量了两个样本点之间的距离，分母部分的 $\tau$ 则是模型的一个超参数，控制了权值随距离下降的速率。可以发现距离越远，权值越小。那距离越远的点在目标函数中起的作用就越小。

## 缺点
对于每一个要查询的点，都要先计算数据集中的每个样本点的权重，然后重新依据整个数据集训练一个线性回归模型出来，计算代价极高。

## 参考
1. 斯坦福机器学习公开课 —— 吴恩达
2. 斯坦福ML公开课笔记3 —— 张雨石

