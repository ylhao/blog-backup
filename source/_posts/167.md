---
title: 循环神经网络：GRU
categories: 机器学习
tags: 机器学习
mathjax: true
date: 2018-05-23 16:55:34
---

## 梯度消失和长距离依赖问题

RNN 中存在的梯度消失问题会导致难以学习到长距离依赖的问题。我的理解是由于梯度消失问题的存在，越早的时刻对参数的修正起到的作用就越小，也就是说模型很难捕捉到长距离依赖关系。

## GRU的基本结构

在 RNN 中，向下传递的是隐层状态，在基本结构中，隐层状态的计算公式如下：

$$
\overrightarrow{h}_t = tanh(W_{hx} \overrightarrow{x}_t + W_{hh} \overrightarrow{h}_{t-1}) \\
$$

## GRU的结构
GRU 引入了 **reset gate** 和 **update gate**。其结构图如下，图中的各符号的意义可以参考“循环神经网络：LSTM”这篇文章。

![](/img/gru1.png)

### reset gate $r_t$
$$
{r}_t = \sigma (W_r) \cdot [h_{t-1}, x_t])
$$

### update gate $z_t$
$$
{z}_t = \sigma (W_z) \cdot [h_{t-1}, x_t])
$$

### 当前时刻的新信息
接下来计算当前时刻的新信息（candidate values, $\widetilde{h}_t$）。这跟 LSTM 中的 candidates values($\widetilde{C}_t$) 是类似的。计算方式如下：
$$
\widetilde{h}_t = tanh(W_h \cdot [r_t \circ h_{t-1}, x_t])
$$

### reset gate 的作用
reset gate 用来控制计算当前时刻的新信息时，保留多少之前的记忆。举个例子来说明一下，假设每个时刻输入的是一个词的话，那么如果 $r_t$ 为 0，那么 $\widetilde{h}_t$ 中就会只包含当前词的信息。

### update gate 的作用
update gate 控制需要从前一时刻的隐藏层状态 $h_{t-1}$ 中忘记多少信息，同时控制需要将多少当前时刻的新信息加入到隐藏层状态中。
$$
h_t = (1-z_t) \circ h_{t-1} + z_t \circ widetilde{h}_t
$$

### 最后的总结
reset gate 允许模型丢弃一些和未来无关的信息，如果reset gate接近0，那么之前的隐藏层信息就会丢弃。
update gate 控制当前时刻的隐藏层输出 $h_t$ 需要保留多少之前的隐藏层信息，若 $z_t$ 接近于 1，相当于我们之前把之前的隐藏层信息拷贝到当前时刻，可以学习长距离依赖。 一般来说那些具有短距离依赖的单元 reset gate 比较活跃，具有长距离依赖的单元 update gate 比较活跃。


## 参考
1. [CS224d笔记4续——RNN隐藏层计算之GRU和LSTM](https://wugh.github.io/posts/2016/03/cs224d-notes4-recurrent-neural-networks-continue/)
2. [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
3. 深度学习工程师微专业 —— 吴恩达
4. [三次简化一张图: 一招理解LSTM/GRU门控机制](https://zhuanlan.zhihu.com/p/28297161)

