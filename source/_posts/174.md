---
title: KNN算法（一）
date: 2018-05-04 14:51:11
categories: 机器学习
tags: 机器学习
mathjax: true
---

## 预备
- 输入空间 $\chi \subseteq R^n$
- 训练集 $T=\{(x_1, y_1), (x_2, y_2), \cdots, (x_N, y_N)\}$
- 输出空间 $\gamma = \{c_1, c_2, \cdots , c_k\}$

## K近邻法基本思想
K 近邻法并不具有显式的学习过程，K 近邻法实际上是利用训练集数据对特征空间进行划分，并作为其分类的“模型”。给定一个训练集，对于新的输入实例，首先在训练集中找到与该实例最近的 K 个实例，然后统计这 K 个实例的多数属于哪个类，就把该实例划分为哪个类。

## K近邻法的三个要素
经过上面的讨论，我们可以总结出 K 近邻法的三个要素：
- K 值的选择
- 距离的度量
- 分类决策的规则

## K 值的选择
首先考虑一个极端情况，当 K 值为 1 时，此时的 K 近邻算法又称为最邻近算法，这种情况下，很容易发生**过拟合**，很容易将一些噪声学习到模型中（很容易将实例判定为噪声类别）。  
我们再考虑另一种极端情况，当 K 值为 N 时，此时不管你输入的实例是什么类别，最终模型都会将该实例判定为模型中实例最多的类别。也就是这种情况下，很容易发生**欠拟合**。
通过以上两个极端情况，可以理解到 K 值越大，模型越简单，鲁棒性越好（容错率越高），越容易欠拟合。

## 距离的度量
- 闵可夫斯基距离
- 曼哈顿距离
- 欧式距离
- 切比雪夫距离

设有两个向量：
$$
x_i = (x_i^{(1)},x_i^{(2)},x_i^{(3)},\cdots,x_i^{(n)})\\
x_j = (x_j^{(1)},x_j^{(2)},x_j^{(3)},\cdots,x_j^{(n)})
$$
闵可夫斯基距离的定义如下：
$$
d_{(x_i, x_j)} = \Big(\sum_{l=1}^n \lvert x_i^{(l)} - x_j^{(l)} \rvert^p \Big)^{\frac{1}{p}}
$$
当 $p = 1$ 时就是曼哈顿距离，当 $p = 2$ 时就是欧式距离，当 $p=\infty$ 时就是切比雪夫距离，这里我们使用欧式距离进行度量。

## 分类决策的规则
这里用的是多数表决的决策规则，很直观~，关于对多数表决规则的解释可以参考《统计学习方法》这本书的 3.2.4 小节（多数表决规则等价于经验风险最小化）。

## 特征归一化
为了让各个特征在分类的时候同等重要，我们需要将各个特征进行归一化。

## 参考
- 《统计学习方法》——李航
- [一文搞懂k近邻（k-NN）算法（一）](https://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&mid=2247483771&idx=1&sn=e8cc00032205a73584867e85aa509a64&chksm=ebb439afdcc3b0b9bb4446bc874def2962886b6db112171c2c8fd73d1dc5d51ad27fb49d5ffd&scene=21#wechat_redirect) —— 忆臻

