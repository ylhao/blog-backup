---
title: LSI算法理论篇
categories: 机器学习
tags: 
- 机器学习
- NLP
mathjax: true
date: 2018-05-17 00:06:46
---

## LSI 简介
潜在语义索引（Latent Semantic Indexing，以下简称 LSI），有的文章也叫潜在语义分析（Latent Semantic  Analysis，LSA）。在这篇文章里，我选择叫潜在语义索引。LSI 是一种主题模型，LSI 基于 SVD 得到文本的主题。

## SVD
将一个 $m \times n$ 矩阵分解的公式如下：
$$
A_{m \times n} = U_{m \times m}\Sigma_{m \times n} V^T_{n \times n}
$$
有时为了降低矩阵的维度，上述公式可以写成以下形式：
$$
A_{m \times n} \approx U_{m \times k}\Sigma_{k \times k} V^T_{k \times n}
$$

## LSI 中 SVD 的解释
假设我们有 m 个样本，每个文本有 n 个词，$A_{ij}$ 对应的是第 i 个文本的第 j 个词的特征值，我们最常用的是基于**预处理后的标准化的** TF-IDF 值。k 是我们的主题数（一般要比文本数少），SVD 分解后，$U_{il}$ 对应第 i 个文本和第 l 个主题的相关程度。$V_{jm}$ 对应第 j 个关键词和第 m 个词的词义相关度。$\Sigma_{lm}$ 对应第 l 个主题和第 m 个词的词义相关度。
这样我们通过一次 SVD，就可以得到文档和主题的相关度，关键词和词的相关度以及关键词和主题的相关度。

## 文本相似度计算
如果我们要做文本相似度计算，那么我们可以分以下几步：
1. 文本预处理，构造文本的 TF-IDF 向量。
2. 所有文本的 TF-IDF 向量构成一个 $m \times n$ 维的矩阵，m 代表文本个数，$n$ 代表词典长度。
3. 对这个 $m \times n$ 维的矩阵进行 SVD。
4. 根据分解得到的矩阵 $U_{m \times k}$ 进行相似度的计算（例如可以采用余弦相似度等）。
5. 以上所有的步骤合起来就可以看成基于主题的文本相似度分析。

## LSI 问题
1. SVD 计算非常的耗时，特别是当词和文本数都非常大的时候，对于这样的高维度矩阵做奇异值分解是非常难的。
2. 主题值的选取对结果的影响非常大，很难选择合适的 k 值。
3. LSI 得到的不是一个概率模型，缺乏统计基础，结果难以直观的解释。

## 改进
1. 主题模型非负矩阵分解（NMF）可以解决矩阵分解的速度问题。
2. 层次狄利克雷过程（HDP）可以自动选择主题个数。
3. 后人们提出了 pLSI（也叫 pLSA）和隐含狄利克雷分布（LDA）这类基于概率分布的主题模型。

## LSI 现状
LSI 已经基本不再使用，对于一些规模较小的问题，如果想快速粗粒度的找出一些主题分布的关系，则 LSI 是比较好的一个选择，其他时候，如果你需要使用主题模型，推荐使用 LDA 和 HDP。

## 参考
1. [文本主题模型之潜在语义索引(LSI)](http://www.cnblogs.com/pinard/p/6805861.html) —— 刘建平

